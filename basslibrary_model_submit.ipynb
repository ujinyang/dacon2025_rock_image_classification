{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7d32fc-6550-4880-bc47-dfea0e68fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !apt install -y unzip imagemagick libopencv-dev\n",
    "# !pip install pandas scikit-learn opencv-python wand wandb accelerate transformers timm bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1a6667-a115-49b5-91b8-bd5fbad1e5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 12:47:54 [INFO] program started\n"
     ]
    }
   ],
   "source": [
    "import os, gc, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# half 에서 메모리사용량과 관련이 있을지?\n",
    "torch.set_float32_matmul_precision('high')  # or 'medium' | 'high'\n",
    "\n",
    "os.environ['WANDB_MODE']='online'\n",
    "os.environ['WANDB_PROJECT']='basslibrary240210'\n",
    "os.environ['WANDB_MODE']='offline'\n",
    "\n",
    "######## logger ########\n",
    "import sys, logging, IPython\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig( handlers=[ logging.StreamHandler(stream=sys.stdout), logging.handlers.RotatingFileHandler(filename='run.log', mode='a', maxBytes=512000, backupCount=4) ] )\n",
    "logging_fomatter = logging.Formatter( '%(asctime)s [%(levelname)-4.4s] %(message)s', datefmt='%m/%d %H:%M:%S' )\n",
    "_ = [ h.setFormatter(logging_fomatter) for h in logger.handlers ]\n",
    "logger.setLevel(logging.INFO)\n",
    "def showtraceback(self, *args, **kwargs):\n",
    "    logger.exception('-------Exception----------')\n",
    "IPython.core.interactiveshell.InteractiveShell.showtraceback = showtraceback\n",
    "logger.info('program started')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94aba304-5340-4ee7-bc1e-968aea5c73af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 12:47:54 [INFO] {'OVERSAMPLING': True, 'GRADIENT_CHECKPOINT': False, 'ACCUMULATION_STEPS': 1, 'MAX_CLASSES': 7, 'SEED': 42, 'N_SPLIT': 5, 'LABEL_SMOOTHING': 0.05, 'RESIZE_TO_FIT': False, 'OPTIMIZER': 'AdamW', 'INTERPOLATION': 'robidouxsharp', 'PRECISION': '16', 'MODEL_NAME': 'timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k', 'IMG_SIZE': 448, 'BATCH_SIZE': 16, 'LR': [2e-05, 2e-06], 'IMG_TRAIN_SIZE': 448}\n"
     ]
    }
   ],
   "source": [
    "CFG = {}\n",
    "CFG['OVERSAMPLING'] = True\n",
    "CFG['GRADIENT_CHECKPOINT'] = False  ## gradient_checkpoint 사용여부\n",
    "CFG['ACCUMULATION_STEPS'] = 1  ## 숫자를 늘리면, weights가 메모리에 여러벌 존재해야함.\n",
    "CFG['MAX_CLASSES'] = 7 ## len(train_df.label.unique())\n",
    "CFG['SEED'] = 42\n",
    "CFG['N_SPLIT'] = 5\n",
    "CFG['LABEL_SMOOTHING'] = 0.05\n",
    "CFG['RESIZE_TO_FIT'] = False\n",
    "CFG['OPTIMIZER'] = 'AdamW'\n",
    "CFG['INTERPOLATION'] = 'robidouxsharp' ## best..\n",
    "CFG['PRECISION'] = '16' # '16'\n",
    "# #----------------------------------\n",
    "CFG['MODEL_NAME'] = \"timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k\"\n",
    "CFG['IMG_SIZE'] = 448\n",
    "CFG['BATCH_SIZE'] = 48 if CFG['GRADIENT_CHECKPOINT'] else 16 # 20(10GB)\n",
    "CFG['LR'] = [ 1e-5 / float(8) * CFG['BATCH_SIZE'] * CFG['ACCUMULATION_STEPS'], 1e-6  / float(8) * CFG['BATCH_SIZE'] * CFG['ACCUMULATION_STEPS']] # 1e-6 => 1e-5\n",
    "######################################\n",
    "if 'IMG_TRAIN_SIZE' not in CFG:\n",
    "    CFG['IMG_TRAIN_SIZE'] = CFG['IMG_SIZE']\n",
    "logger.info(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9f382cc-63c5-4ea3-94bd-48a68e107ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 12:47:54 [INFO] device='cuda'\n",
      "05/30 12:47:54 [INFO] dtype=torch.float32\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "logger.info(f'{device=}')\n",
    "logger.info(f'dtype={torch.get_default_dtype()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2baa506-89ad-4a99-9071-9b64c3eb1be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 12:47:54 [INFO] seed_everything : seed=42\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    logger.info(f'seed_everything : {seed=}')\n",
    "\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b351e58c-d229-47a4-827a-d34d26ff2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('train.csv'):\n",
    "    train_list = glob('train/*/*.jpg')\n",
    "    train_df = pd.DataFrame(train_list)\n",
    "    train_df['rock_type'] = train_df[0].apply(lambda x: x.split(os.path.sep)[1])\n",
    "    train_df['img_path'] = train_df[0].apply(lambda x: x.replace('open' + os.path.sep,'.' + os.path.sep) )\n",
    "    train_df['ID'] = train_df.index\n",
    "    train_df[['ID','img_path','rock_type']].to_csv('train.csv', index=False)\n",
    "    logger.info('train.csv saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f379871e-7a77-4c39-a461-37e37150e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = CFG['IMG_SIZE']\n",
    "\n",
    "train_transform_list = [\n",
    "    v2.TrivialAugmentWide(interpolation=v2.InterpolationMode.BICUBIC), \n",
    "    v2.RandomHorizontalFlip(), ## 성능이 좋아짐.\n",
    "    v2.RandomVerticalFlip(), ## ??\n",
    "    v2.ToImage(), v2.ToDtype( torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "]\n",
    "train_transform = v2.Compose(train_transform_list )\n",
    "test_transform = v2.Compose( [\n",
    "    v2.ToImage(), v2.ToDtype( torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3308498d-0efa-466c-a7f2-60c0095c0c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, load_img_size, shuffle=False, transforms=None, interpolation='robidouxsharp' ):\n",
    "        self.df = pd.DataFrame({'img_path_list': img_path_list})\n",
    "        self.interpolation = interpolation\n",
    "        self.load_img_size = load_img_size\n",
    "        logger.info(f'load_img_size={load_img_size}')\n",
    "        if label_list is not None:\n",
    "            self.df['label_list'] = label_list\n",
    "        if shuffle:\n",
    "            self.df = self.df.sample(frac=1.0).reset_index(drop=True)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    # numpy or PIL Image => PIL Image\n",
    "    def get_interpolated_image(self, img, new_image_size):\n",
    "        if isinstance(new_image_size, int):\n",
    "            new_image_size = [ new_image_size, new_image_size ]\n",
    "        if self.interpolation == 'pil_lanczos':\n",
    "            if isinstance(img, np.ndarray ):\n",
    "                img = Image.fromarray(img)\n",
    "            return img.resize( new_image_size, Image.LANCZOS )\n",
    "        elif self.interpolation == 'cv2_lanczos4':\n",
    "            if not isinstance(img, np.ndarray ):\n",
    "                img = np.array(img)\n",
    "            import cv2\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "            img = cv2.resize(img, new_image_size, interpolation=cv2.INTER_LANCZOS4) # 픽셀 크기 지정\n",
    "            img = cv2.cvtColor(np.array(img), cv2.COLOR_BGR2RGB)\n",
    "            return Image.fromarray(img)\n",
    "        else:\n",
    "            if not isinstance(img, np.ndarray ):\n",
    "                img = np.array(img)\n",
    "            from wand import image\n",
    "            with image.Image.from_array(img) as src:\n",
    "                src.resize( *new_image_size, filter=self.interpolation )\n",
    "                return Image.fromarray(np.array(src))\n",
    "\n",
    "    def numpy_sqaure_array(self, img):\n",
    "        h,w,c = np.array(img).shape\n",
    "        if h>w:\n",
    "            sq_img = np.full((h, h, c), fill_value=0, dtype=np.uint8)\n",
    "            sq_img[:,(h-w)//2:(h-w)//2+w] = img\n",
    "        else:\n",
    "            sq_img = np.full((w, w, c), fill_value=0, dtype=np.uint8)\n",
    "            sq_img[(w-h)//2:(w-h)//2+h,:] = img\n",
    "        return sq_img\n",
    "        \n",
    "    def get_image_from_index(self, index, img_size, exact = False ):\n",
    "        img_path = self.df.img_path_list[index]\n",
    "        img = Image.open(img_path)\n",
    "        W, H = img.size\n",
    "        if ('label_list' in self.df.columns) and ( index % 2 == 0 ):\n",
    "            img = img.crop((W//4,H//4,W//4+W//2,H//4+H//2)) \n",
    "            W, H = img.size\n",
    "\n",
    "        scale = max(int((img_size+W-1)//W), int((img_size+W-1)//H))\n",
    "        if (exact == True) or (W <= img_size) or (H <= img_size):\n",
    "            img = self.get_interpolated_image(img, img_size )\n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = self.get_image_from_index( index, self.load_img_size, exact = True)\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        if 'label_list' in self.df.columns:\n",
    "            label = self.df.label_list[index]\n",
    "            return { 'pixel_values': image, 'label': label }\n",
    "        else:\n",
    "            return { 'pixel_values': image }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "009ce765-a5de-4757-8ed5-7055dccb566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ref: https://github.com/katsura-jp/pytorch-cosine-annealing-with-warmup/blob/master/cosine_annealing_warmup/scheduler.py\n",
    "import math\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
    "    \"\"\"\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        first_cycle_steps (int): First cycle step size.\n",
    "        cycle_mult(float): Cycle steps magnification. Default: -1.\n",
    "        max_lr(float): First cycle's max learning rate. Default: 0.1.\n",
    "        min_lr(float): Min learning rate. Default: 0.001.\n",
    "        warmup_steps(int): Linear warmup step size. Default: 0.\n",
    "        gamma(float): Decrease rate of max learning rate by cycle. Default: 1.\n",
    "        last_epoch (int): The index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 optimizer : torch.optim.Optimizer,\n",
    "                 first_cycle_steps : int,\n",
    "                 cycle_mult : float = 1.,\n",
    "                 max_lr : float = 1e-5,\n",
    "                 min_lr : float = 1e-10,\n",
    "                 warmup_steps : int = 0,\n",
    "                 gamma : float = 1.,\n",
    "                 last_epoch : int = -1\n",
    "        ):\n",
    "        assert warmup_steps < first_cycle_steps\n",
    "        \n",
    "        self.first_cycle_steps = first_cycle_steps # first cycle step size\n",
    "        self.cycle_mult = cycle_mult # cycle steps magnification\n",
    "        self.base_max_lr = max_lr # first max learning rate\n",
    "        self.max_lr = max_lr # max learning rate in the current cycle\n",
    "        self.min_lr = min_lr # min learning rate\n",
    "        self.warmup_steps = warmup_steps # warmup step size\n",
    "        self.gamma = gamma # decrease rate of max learning rate by cycle\n",
    "        \n",
    "        self.cur_cycle_steps = first_cycle_steps # first cycle step size\n",
    "        self.cycle = 0 # cycle count\n",
    "        self.step_in_cycle = last_epoch # step size of the current cycle\n",
    "        \n",
    "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "        # set learning rate min_lr\n",
    "        self.init_lr()\n",
    "    \n",
    "    def init_lr(self):\n",
    "        self.base_lrs = []\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = self.min_lr\n",
    "            self.base_lrs.append(self.min_lr)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.step_in_cycle == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.step_in_cycle < self.warmup_steps:\n",
    "            return [(self.max_lr - base_lr)*self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.max_lr - base_lr) \\\n",
    "                    * (1 + math.cos(math.pi * (self.step_in_cycle-self.warmup_steps) \\\n",
    "                                    / (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.step_in_cycle = self.step_in_cycle + 1\n",
    "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
    "                self.cycle += 1\n",
    "                self.step_in_cycle = self.step_in_cycle - self.cur_cycle_steps\n",
    "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
    "        else:\n",
    "            if epoch >= self.first_cycle_steps:\n",
    "                if self.cycle_mult == 1.:\n",
    "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
    "                    self.cycle = epoch // self.first_cycle_steps\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
    "                    self.cycle = n\n",
    "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
    "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** (n)\n",
    "            else:\n",
    "                self.cur_cycle_steps = self.first_cycle_steps\n",
    "                self.step_in_cycle = epoch\n",
    "        \n",
    "        self.max_lr = (self.base_max_lr - self.min_lr) * (self.gamma**self.cycle) + self.min_lr ## min_lr 보다 작아지지 않게 수정\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6314ccb-887f-4f56-8893-7ef930d6af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./train.csv')\n",
    "le = LabelEncoder()\n",
    "train_df['rock_type_int'] = le.fit_transform(train_df['rock_type'])\n",
    "train_df.drop(columns='rock_type', inplace=True)\n",
    "# # 109826\t109826\ttrain/Etc/TRAIN_14569.jpg\tEtc\t28\t23\n",
    "# # 117048\t117048\ttrain/Etc/TRAIN_03346.jpg\tEtc\t34\t32\n",
    "# # 117545\t117545\ttrain/Etc/TRAIN_14567.jpg\tEtc\t28\t23\n",
    "# # 187913\t187913\ttrain/Mud_Sandstone/TRAIN_68767.jpg\tMud_Sandstone\t32\t1\n",
    "# train_df = train_df[ ~train_df.index.isin((109826, 117048, 117545, 187913)) ].reset_index(drop=True).copy()\n",
    "train_df = train_df[ ~train_df.index.isin((187913,)) ].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83f5d9a3-7c5e-4d66-82c0-513947e080db",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CFG['N_SPLIT'], random_state=CFG['SEED'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43ca2015-eee6-4ae6-91b2-e4c1a862bab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_cuda(batch):\n",
    "    inputs = torch.stack([item['pixel_values'] for item in batch]).to(device)\n",
    "    labels = torch.tensor([item['label'] for item in batch]).to(device)\n",
    "    return {'pixel_values': inputs, 'label': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f46fb079-b62b-452a-9e63-aa3fd7ab6155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversampling(df):\n",
    "    max_items = df['rock_type_int'].value_counts().max()\n",
    "    selected_dfs = []\n",
    "    for type_int in df['rock_type_int'].unique():\n",
    "        selected_df = df[ df.rock_type_int == type_int ]\n",
    "        if max_items == len(selected_df):\n",
    "            logger.info(f'already max rock_type : {type_int}')\n",
    "            continue\n",
    "        selected_dfs.append( selected_df.sample(max_items - len(selected_df), replace=True ) )\n",
    "    df = pd.concat( [ df, *selected_dfs ], axis=0 ).sample(frac=1).reset_index(drop=True).copy()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b43d120-80f3-445b-b0d4-bdf3b2efa6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, intput_train_fold_df, valid_fold_df, device, validation_steps = 0.25, logging_steps = 10, use_amp=True, filename=''):\n",
    "    \n",
    "    accelerator = None\n",
    "    # accelerator = Accelerator(cpu=(device=='cpu'), mixed_precision='fp16' if use_amp else None, gradient_accumulation_steps=CFG['ACCUMULATION_STEPS'] )\n",
    "    scaler = torch.GradScaler(device, enabled=use_amp)\n",
    "    \n",
    "    logger.info(f'{use_amp=}')\n",
    "\n",
    "    best_score = 0\n",
    "    best_loss  = 1000\n",
    "    best_model = None\n",
    "    MAX_PATIENCE = 5\n",
    "    best_patience = MAX_PATIENCE\n",
    "\n",
    "    # ---------\n",
    "    if CFG['OVERSAMPLING']:\n",
    "        train_fold_df = oversampling(intput_train_fold_df.copy())\n",
    "    else:\n",
    "        train_fold_df = intput_train_fold_df.copy()\n",
    "    # # ---------------------------------------\n",
    "    train_dataset = CustomDataset( train_fold_df['img_path'].values, train_fold_df['rock_type_int'].values, \n",
    "        interpolation=CFG['INTERPOLATION'], load_img_size=CFG['IMG_TRAIN_SIZE'], shuffle=True, transforms=train_transform)\n",
    "    val_dataset = CustomDataset( valid_fold_df['img_path'].values, valid_fold_df['rock_type_int'].values,\n",
    "        interpolation=CFG['INTERPOLATION'], load_img_size=CFG['IMG_SIZE'], shuffle=False, transforms=test_transform)\n",
    "    # # ---------------------------------------\n",
    "    scheduler = None\n",
    "    scheduler = CosineAnnealingWarmupRestarts(\n",
    "        optimizer,\n",
    "        first_cycle_steps=int( (len(train_fold_df) + CFG['BATCH_SIZE'] - 1)//CFG['BATCH_SIZE'] ) // 4,\n",
    "        cycle_mult=1.0, max_lr=CFG['LR'][0] * 2, \n",
    "        min_lr=CFG['LR'][1],\n",
    "        warmup_steps=0, \n",
    "        gamma=0.93,  ## 2024.05.02\n",
    "    )\n",
    "    # ---------------------------------------\n",
    "    train_class_weight = torch.FloatTensor( len(train_fold_df) / CFG[\"MAX_CLASSES\"] / train_fold_df.rock_type_int.value_counts().sort_index() )\n",
    "    valid_class_weight = torch.FloatTensor( len(valid_fold_df) / CFG[\"MAX_CLASSES\"] / valid_fold_df.rock_type_int.value_counts().sort_index() )\n",
    "    logger.info(f'{le.classes_=}')\n",
    "    logger.info(f'{train_class_weight=}')\n",
    "    logger.info(f'{valid_class_weight=}')\n",
    "    loss_fn = nn.CrossEntropyLoss( weight=train_class_weight, label_smoothing=CFG['LABEL_SMOOTHING'], reduction='mean' )#.to(accelerator.device)\n",
    "    checkpoint_filenames = []\n",
    "\n",
    "    # ema 모델은 모델의 weight 한벌을 가지고 있어, 메모리 사용량도 확인해야 함..\n",
    "    ema_model = None  ## 의미가 없을 듯..\n",
    "    ema_decay = np.power(np.e, np.log(0.5)/(validation_steps*MAX_PATIENCE))\n",
    "    ema_model = torch.optim.swa_utils.AveragedModel(model, multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(ema_decay))#.to(accelerator.device)\n",
    "        \n",
    "    # loss_fn = accelerator.prepare( loss_fn )\n",
    "    # model, ema_model = accelerator.prepare( model, ema_model )\n",
    "    # optimizer, scheduler = accelerator.prepare( optimizer, scheduler )\n",
    "    # train_loader, val_loader = accelerator.prepare( train_loader, val_loader )\n",
    "    model = model.to(device)\n",
    "    # model.compile()\n",
    "    ema_model = ema_model.to(device)\n",
    "    # ema_model.compile()\n",
    "    loss_fn = loss_fn.to(device)\n",
    "\n",
    "\n",
    "    for epoch in range(1, 1000):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        pbar_postfix = {}\n",
    "        train_labels = []\n",
    "        train_preds = []\n",
    "        lastlog_tm = time.time()\n",
    "\n",
    "        # # ----------- OVER SAMPLING\n",
    "        if CFG['OVERSAMPLING']: # and (epoch > 1):\n",
    "            train_fold_df = oversampling(intput_train_fold_df.copy())\n",
    "            train_dataset = CustomDataset( train_fold_df['img_path'].values, train_fold_df['rock_type_int'].values, \n",
    "                interpolation=CFG['INTERPOLATION'], load_img_size=CFG['IMG_TRAIN_SIZE'], shuffle=True, transforms=train_transform)\n",
    "            # train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=4)\n",
    "            # train_loader = accelerator.prepare( train_loader )\n",
    "            # 교집합이 없어야 함.\n",
    "            assert (set(train_fold_df['img_path']) & set(valid_fold_df['img_path'])) == set()\n",
    "        ## 데이터 로딩이 문제가 있을거 염려됨..\n",
    "        train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=4)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE']*2, shuffle=False, num_workers=4)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "        # # ----------- ema_testing..\n",
    "        # ema_model.update_parameters(model)\n",
    "        # torch.optim.swa_utils.update_bn(train_loader, ema_model, device if not accelerator else accelerator.device )\n",
    "        # validation(accelerator, ema_model, loss_fn, val_loader, use_amp, desc='Ema validation:' )\n",
    "        # # torch.save( {\"model\": accelerator.unwrap_model(ema_model).state_dict() if accelerator else ema_model.state_dict() }, 'save_filename' )\n",
    "        # # ----------- validation_testing..\n",
    "        # validation(accelerator, model, loss_fn, val_loader, use_amp, desc='Model validation:')\n",
    "\n",
    "        # -----------------------\n",
    "        max_steps = len(train_loader)\n",
    "        if not isinstance(validation_steps, int):\n",
    "            validation_steps = int(max_steps * validation_steps)  ## 절사..\n",
    "            max_steps = (max_steps//validation_steps)*validation_steps\n",
    "            \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "        for i, batch in enumerate(pbar):\n",
    "            if i >= max_steps:\n",
    "                continue\n",
    "            steps = i+1\n",
    "            \n",
    "            with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n",
    "                output = model( batch['pixel_values'].to(device) )\n",
    "                loss = loss_fn(output, batch['label'].to(device) )\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            scaler.unscale_(optimizer)\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.2)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            \n",
    "            # with accelerator.accumulate(model):\n",
    "            #     optimizer.zero_grad()\n",
    "            #     output = model( batch['pixel_values'] )\n",
    "            #     loss = loss_fn(output, batch['label'] )\n",
    "            #     accelerator.backward(loss)\n",
    "\n",
    "            #     # LR 값이 큰경우, 아래 항목이 있어야 함.\n",
    "            #     if accelerator.sync_gradients:\n",
    "            #         accelerator.clip_grad_norm_(model.parameters(), max_norm=0.2)\n",
    "                    \n",
    "            #     train_labels += batch['label'].detach().cpu().numpy().tolist()\n",
    "            #     train_preds += output.detach().argmax(1).cpu().numpy().tolist()\n",
    "                \n",
    "            #     optimizer.step()\n",
    "            #     if scheduler is not None:\n",
    "            #         scheduler.step()\n",
    "\n",
    "            # train 변수 초기화\n",
    "            if (steps-1) % validation_steps == 0:\n",
    "                train_loss = []\n",
    "                train_labels = []\n",
    "                train_preds = []\n",
    "                \n",
    "            # 변수값 업데이트..\n",
    "            train_loss.append(loss.item())\n",
    "            train_labels += batch['label'].detach().cpu().numpy().tolist()\n",
    "            train_preds += output.detach().argmax(1).cpu().numpy().tolist()\n",
    "            # # 사용안하는 메모리 초기화..\n",
    "            # loss = None\n",
    "            # output = None\n",
    "            # batch = None\n",
    "            \n",
    "            if ema_model is not None:\n",
    "                if accelerator is not None:\n",
    "                    if accelerator.sync_gradients:\n",
    "                        ema_model.update_parameters(model)\n",
    "                else:\n",
    "                    # with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n",
    "                    ema_model.update_parameters(model)\n",
    "                    \n",
    "\n",
    "            if steps % logging_steps == 0:\n",
    "                _train_loss = float(np.mean(train_loss))\n",
    "                _train_f1_all = f1_score(train_labels, train_preds, average=None).tolist()\n",
    "                \n",
    "                pbar_postfix.update({\n",
    "                    't_loss0': train_loss[-1], \n",
    "                    'lr': optimizer.param_groups[0][\"lr\"],\n",
    "                    'score' : float(np.mean(_train_f1_all)),\n",
    "                    # **{ f'f1_{i}':v for i, v in enumerate(_train_f1_all) }\n",
    "                } )\n",
    "                pbar.set_postfix( pbar_postfix )\n",
    "                # 수정되어야 함..\n",
    "                if ( time.time() - lastlog_tm ) > 60:\n",
    "                    t_f1_raw_str = ', '.join([ f'{float(v):.3f}' for v in _train_f1_all ])\n",
    "                    logger.info(f'eps={epoch:d}, steps={steps}/{max_steps}, lr={optimizer.param_groups[0][\"lr\"]:.3g}, t_loss={_train_loss:.4f}, t_f1r=[{t_f1_raw_str}], t_f1={float(np.mean(_train_f1_all)):.3f}' )\n",
    "                    lastlog_tm = time.time()\n",
    "                \n",
    "                run.log({\n",
    "                    \"epoch\": epoch, \n",
    "                    \"step\": steps,\n",
    "                    \"train\":{\n",
    "                        \"loss\": train_loss[-1],\n",
    "                        \"f1\": np.mean(_train_f1_all),\n",
    "                        **{ f'f1.{i}({le.classes_[i][:3].lower()})': v for i, v in enumerate(_train_f1_all) }\n",
    "                    }, \n",
    "                    \"lr\": optimizer.param_groups[0][\"lr\"]\n",
    "                }, step=(epoch-1)*max_steps+steps)\n",
    "                \n",
    "            if steps % validation_steps == 0:\n",
    "                _val_loss, _val_score, _val_score_all = validation(accelerator, model, loss_fn, val_loader, use_amp)\n",
    "                _val_score_all = _val_score_all.tolist()\n",
    "                _val_score = float(_val_score)\n",
    "                _train_loss = float(np.mean(train_loss))\n",
    "                \n",
    "                best_score_mark = '*' if best_score < _val_score else ' '\n",
    "                best_loss_mark = '*' if best_loss > _val_loss else ' '\n",
    "                pbar_postfix.update({\n",
    "                    'lr': optimizer.param_groups[0][\"lr\"], \n",
    "                    't_loss': _train_loss,\n",
    "                    'v_loss': _val_loss, \n",
    "                    'v_f1': _val_score,\n",
    "                })\n",
    "                pbar.set_postfix( pbar_postfix )\n",
    "                v_f1_raw_str = ', '.join([ f'{float(v):.3f}' for v in _val_score_all ])\n",
    "                logger.info(f'eps={epoch:d}, steps={steps}/{max_steps}, lr={optimizer.param_groups[0][\"lr\"]:.3g}, t_loss={_train_loss:.4f}, v_f1r=[{v_f1_raw_str}], v_loss={_val_loss:.4f}{best_loss_mark}, v_f1={float(_val_score):.4f}{best_score_mark}' )\n",
    "                             \n",
    "                run.log({\n",
    "                    \"epoch\": epoch, \"step\": steps,\n",
    "                    \"train\": { \n",
    "                        \"avg_loss\": _train_loss\n",
    "                    }, \n",
    "                    \"valid\": { \n",
    "                        \"avg_loss\": _val_loss,\n",
    "                        \"score\": float(np.mean(_val_score_all)),\n",
    "                        **{ f'f1.{i}({le.classes_[i][:3].lower()})': v for i, v in enumerate(_val_score_all) }\n",
    "                    },\n",
    "                    \"lr\": optimizer.param_groups[0][\"lr\"] \n",
    "                }, step=(epoch-1)*max_steps+steps)\n",
    "                \n",
    "                if ( best_score < _val_score ):\n",
    "                    if ( _val_score - best_score ) > 0.001:\n",
    "                        best_score = _val_score\n",
    "                        best_patience = MAX_PATIENCE\n",
    "                    best_model = accelerator.unwrap_model( model ) if accelerator is not None else model\n",
    "                    \n",
    "                    ## saving..\n",
    "                    if filename is not None and len(filename) != 0:\n",
    "                        checkpoint_filenames.append(\n",
    "                            filename.format(epoch=epoch, val_loss=_val_loss, val_score=_val_score) + '.ckpt' )\n",
    "                        if best_score > 0.89:\n",
    "                            os.makedirs(os.path.dirname(checkpoint_filenames[-1]), exist_ok=True)\n",
    "                            torch.save( {\"model\": best_model.state_dict() }, checkpoint_filenames[-1] )\n",
    "                            logger.info( f'{checkpoint_filenames[-1]} : saved.' )\n",
    "                            _ = [ os.path.exists(fname) and os.remove(fname) for fname in checkpoint_filenames[:-1] ]\n",
    "                            checkpoint_filenames = checkpoint_filenames[-1:]\n",
    "                    \n",
    "                    ## 추가적으로 비교함..\n",
    "                    if best_loss > _val_loss:\n",
    "                        best_loss = _val_loss\n",
    "                elif ( best_loss > _val_loss ) and (( best_loss - _val_loss ) > 0.001 ):\n",
    "                    best_loss = _val_loss\n",
    "                    best_patience = MAX_PATIENCE\n",
    "                elif best_patience > 0:\n",
    "                    best_patience -= 1\n",
    "                else:\n",
    "                    logger.info(f'NO_MORE_TRAINING, {best_score=:.4f}')\n",
    "                    if ema_model is not None:\n",
    "                        # ## EMA --------------------\n",
    "                        torch.optim.swa_utils.update_bn(train_loader, ema_model, device )\n",
    "                        ema_val_loss, ema_val_score, _ = validation(accelerator, ema_model, loss_fn, val_loader, use_amp)\n",
    "                        logger.info(f'EMA ::: ema_v_loss={ema_val_loss:.4f}, ema_v_f1={ema_val_score:.4f}')\n",
    "                        run.log({'ema_v_loss': ema_val_loss, 'ema_v_f1': ema_val_score })\n",
    "                        \n",
    "                        save_filename = filename.format(epoch=epoch, val_loss=ema_val_loss, val_score=ema_val_score) + '-ema.ckpt'\n",
    "                        torch.save( {\"model\": accelerator.unwrap_model(ema_model).state_dict() if accelerator is not None else ema_model.state_dict() }, save_filename )\n",
    "                        logger.info( f'{save_filename} : (ema) saved.' )\n",
    "                        # ##========================\n",
    "                    if not os.path.exists(checkpoint_filenames[-1]):\n",
    "                        os.makedirs(os.path.dirname(checkpoint_filenames[-1]), exist_ok=True)\n",
    "                        torch.save( {\"model\": best_model.state_dict() }, checkpoint_filenames[-1] )\n",
    "                        logger.info( f'{checkpoint_filenames[-1]} : saved.' )\n",
    "                        _ = [ os.path.exists(fname) and os.remove(fname) for fname in checkpoint_filenames[:-1] ]\n",
    "                        checkpoint_filenames = checkpoint_filenames[-1:]\n",
    "                    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e8cf444-d29c-41dc-8eaa-65c6b56d952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(accelerator, model, loss_fn, val_loader, use_amp=False, desc=None):\n",
    "    save_training = model.training\n",
    "    model.eval()\n",
    "    \n",
    "    val_loss = []\n",
    "    preds, true_labels = [], []\n",
    "\n",
    "    last_log_tm = time.time()\n",
    "    with torch.no_grad():\n",
    "        max_steps = len(val_loader)\n",
    "        for i, batch in enumerate(tqdm(val_loader, desc=desc)):\n",
    "            if accelerator is not None:\n",
    "                pred = model(batch['pixel_values'])\n",
    "                loss = loss_fn(pred, batch['label'])\n",
    "            else:\n",
    "                with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n",
    "                    pred = model(batch['pixel_values'].to(device) )\n",
    "                    loss = loss_fn(pred, batch['label'].to(device) )\n",
    "                \n",
    "            true_labels += batch['label'].detach().cpu().numpy().tolist()\n",
    "            preds += pred.detach().argmax(1).cpu().numpy().tolist()\n",
    "            val_loss.append(loss.cpu().item())\n",
    "\n",
    "            if (time.time() - last_log_tm) > 60:\n",
    "                _val_f1_all = f1_score(true_labels, preds, average=None ).tolist()\n",
    "\n",
    "                v_f1_raw_str = ', '.join([ f'{float(v):.3f}' for v in _val_f1_all ])\n",
    "                logger.info(f'steps={i+1}/{max_steps}, v_loss={float(np.mean(val_loss)):.4f}, v_f1r=[{v_f1_raw_str}], v_f1={float(np.mean(_val_f1_all)):.3f}' )\n",
    "                last_log_tm = time.time()\n",
    "            \n",
    "        _val_loss = np.mean(val_loss)\n",
    "        _val_scores = f1_score(true_labels, preds, average=None ) #'macro')\n",
    "    ## return_to_train..\n",
    "    if save_training:\n",
    "        model.train()\n",
    "    return _val_loss, np.mean(_val_scores), _val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "128488dc-7fc7-49d9-9f56-ff192dd042dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(model, test_loader, device):\n",
    "    model = model.to(device)\n",
    "    save_training = model.training\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            pred = model(batch['pixel_values'].to(device, dtype=torch.float32 ))  ## F.softmax(output) ## 의미는 없을 듯.\n",
    "            preds += pred.detach().cpu().numpy().tolist()\n",
    "    if save_training:\n",
    "        model.train()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d43527d-51b5-4017-9ad0-3018d820f686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model, model_shape):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.gelu = nn.GELU()\n",
    "        self.clf = nn.Linear(model_shape[1], CFG['MAX_CLASSES'])\n",
    "        # 초기화, 가장 좋은 결과가 있었던 kaiming으로 구성함.\n",
    "        torch.nn.init.kaiming_uniform_(self.clf.weight.data)\n",
    "        self.clf.bias.data.fill_(0)\n",
    "        logger.info(f\"clf layer: nn.Linear({model_shape[1]}, {CFG['MAX_CLASSES']}) added and initialize.\")\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = x.pooler_output\n",
    "        x = self.gelu(x)\n",
    "        x = self.clf(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2f51100-bcf4-4528-bf60-41adfc30d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name):\n",
    "    import timm\n",
    "    from transformers import AutoModel, AutoModelForImageClassification, AutoConfig\n",
    "\n",
    "    logger.info(f'create_model: {model_name}')\n",
    "    if '/' not in model_name:\n",
    "        model_name = 'timm/' + model_name\n",
    "    #        \n",
    "    if model_name.startswith('./'):\n",
    "        import nextvit\n",
    "        model = timm.create_model('nextvit_large', pretrained=True, checkpoint_path=model_name)\n",
    "    elif model_name.startswith('facebook/hiera_'):\n",
    "        from hiera import Hiera  ## pip install hiera-transformer\n",
    "        model = Hiera.from_pretrained(model_name)\n",
    "    elif model_name.startswith('timm/'):\n",
    "        # model = CustomModel( timm.create_model( model_name, pretrained=True ) )\n",
    "        try:\n",
    "            model = timm.create_model( model_name, pretrained=True, img_size=CFG['IMG_SIZE'], num_classes=CFG['MAX_CLASSES'] )\n",
    "        except:\n",
    "            model = timm.create_model( model_name, pretrained=True, num_classes=CFG['MAX_CLASSES'] )\n",
    "\n",
    "        ## initlialize model weights\n",
    "        ## normal_ : 0.7334, unifiom_ : 0.7862, xavier_uniform_ : 0.7971, kaiming_uniform_: 0.7982\n",
    "        try:\n",
    "            torch.nn.init.kaiming_uniform_(model.head.weight.data)\n",
    "            model.head.bias.data.fill_(0)\n",
    "        except:\n",
    "            torch.nn.init.kaiming_uniform_(model.head.fc.weight.data)\n",
    "            model.head.fc.bias.data.fill_(0)\n",
    "            # logger.info('initlialize model weights ( model.head.weight.data, model.head.bias.data )' )\n",
    "        return model\n",
    "    else:\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    # if CFG['PRECISION'] == '16':\n",
    "    #     model.half()\n",
    "    output = model( torch.randn((1,3,CFG['IMG_SIZE'],CFG['IMG_SIZE'])) ) ## model_output check..\n",
    "    if not isinstance(output, torch.Tensor):\n",
    "        output = output.pooler_output\n",
    "    assert (len(output.shape) == 2) and (output.shape[1] >= CFG['MAX_CLASSES'])\n",
    "    if output.shape[1] != CFG['MAX_CLASSES']:\n",
    "        model = CustomModel( model, output.shape )\n",
    "    model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f093b6-d735-461c-9659-7dc70d8863f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 12:47:55 [INFO] fold_idx=0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 12:47:56 [INFO] create_model: timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k\n",
      "05/30 12:47:57 [INFO] Loading pretrained weights from Hugging Face hub (timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k)\n",
      "05/30 12:47:58 [INFO] [timm/eva02_base_patch14_448.mim_in22k_ft_in22k_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "05/30 12:47:58 [INFO] Missing keys (head.weight, head.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "05/30 12:47:59 [INFO] model compiled\n",
      "05/30 12:47:59 [INFO] create optimizer : torch.optim.adamw\n",
      "05/30 12:47:59 [INFO] use_amp=True\n",
      "05/30 12:47:59 [INFO] already max rock_type : 4\n",
      "05/30 12:47:59 [INFO] load_img_size=448\n",
      "05/30 12:47:59 [INFO] load_img_size=448\n",
      "05/30 12:47:59 [INFO] le.classes_=array(['Andesite', 'Basalt', 'Etc', 'Gneiss', 'Granite', 'Mud_Sandstone',\n",
      "       'Weathered_Rock'], dtype=object)\n",
      "05/30 12:47:59 [INFO] train_class_weight=tensor([1., 1., 1., 1., 1., 1., 1.])\n",
      "05/30 12:47:59 [INFO] valid_class_weight=tensor([1.2393, 2.0249, 3.4069, 0.7345, 0.5843, 0.6068, 1.4605])\n",
      "05/30 12:47:59 [INFO] already max rock_type : 4\n",
      "05/30 12:47:59 [INFO] load_img_size=448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8457c32b3e4229895b2962255bd8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/32524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 12:49:02 [INFO] eps=1, steps=220/32524, lr=3.99e-05, t_loss=1.7288, t_f1r=[0.287, 0.520, 0.259, 0.370, 0.572, 0.366, 0.476], t_f1=0.407\n",
      "05/30 12:50:03 [INFO] eps=1, steps=450/32524, lr=3.97e-05, t_loss=1.4532, t_f1r=[0.387, 0.658, 0.312, 0.451, 0.684, 0.434, 0.577], t_f1=0.500\n",
      "05/30 12:51:04 [INFO] eps=1, steps=680/32524, lr=3.93e-05, t_loss=1.3452, t_f1r=[0.436, 0.709, 0.341, 0.506, 0.719, 0.448, 0.616], t_f1=0.539\n",
      "05/30 12:52:05 [INFO] eps=1, steps=910/32524, lr=3.88e-05, t_loss=1.2819, t_f1r=[0.471, 0.742, 0.353, 0.534, 0.738, 0.467, 0.633], t_f1=0.563\n",
      "05/30 12:53:06 [INFO] eps=1, steps=1140/32524, lr=3.82e-05, t_loss=1.2390, t_f1r=[0.487, 0.758, 0.369, 0.548, 0.749, 0.484, 0.652], t_f1=0.578\n",
      "05/30 12:54:07 [INFO] eps=1, steps=1370/32524, lr=3.74e-05, t_loss=1.2061, t_f1r=[0.502, 0.774, 0.386, 0.566, 0.760, 0.491, 0.662], t_f1=0.591\n",
      "05/30 12:55:09 [INFO] eps=1, steps=1600/32524, lr=3.65e-05, t_loss=1.1816, t_f1r=[0.518, 0.787, 0.399, 0.577, 0.764, 0.497, 0.666], t_f1=0.601\n",
      "05/30 12:56:10 [INFO] eps=1, steps=1830/32524, lr=3.54e-05, t_loss=1.1608, t_f1r=[0.531, 0.793, 0.413, 0.586, 0.768, 0.504, 0.672], t_f1=0.610\n",
      "05/30 12:57:12 [INFO] eps=1, steps=2060/32524, lr=3.43e-05, t_loss=1.1428, t_f1r=[0.542, 0.800, 0.422, 0.593, 0.774, 0.517, 0.679], t_f1=0.618\n",
      "05/30 12:58:13 [INFO] eps=1, steps=2290/32524, lr=3.3e-05, t_loss=1.1257, t_f1r=[0.548, 0.806, 0.428, 0.599, 0.780, 0.523, 0.687], t_f1=0.624\n",
      "05/30 12:59:15 [INFO] eps=1, steps=2520/32524, lr=3.17e-05, t_loss=1.1093, t_f1r=[0.557, 0.813, 0.434, 0.608, 0.784, 0.531, 0.692], t_f1=0.631\n",
      "05/30 13:00:16 [INFO] eps=1, steps=2750/32524, lr=3.02e-05, t_loss=1.0974, t_f1r=[0.566, 0.816, 0.443, 0.613, 0.786, 0.534, 0.694], t_f1=0.636\n",
      "05/30 13:01:18 [INFO] eps=1, steps=2980/32524, lr=2.87e-05, t_loss=1.0838, t_f1r=[0.573, 0.822, 0.448, 0.620, 0.789, 0.542, 0.699], t_f1=0.642\n",
      "05/30 13:02:20 [INFO] eps=1, steps=3210/32524, lr=2.72e-05, t_loss=1.0748, t_f1r=[0.576, 0.826, 0.455, 0.623, 0.793, 0.546, 0.699], t_f1=0.646\n",
      "05/30 13:03:22 [INFO] eps=1, steps=3440/32524, lr=2.55e-05, t_loss=1.0648, t_f1r=[0.582, 0.830, 0.460, 0.628, 0.798, 0.549, 0.702], t_f1=0.650\n",
      "05/30 13:04:23 [INFO] eps=1, steps=3670/32524, lr=2.39e-05, t_loss=1.0533, t_f1r=[0.592, 0.834, 0.464, 0.633, 0.799, 0.558, 0.705], t_f1=0.655\n",
      "05/30 13:05:25 [INFO] eps=1, steps=3900/32524, lr=2.22e-05, t_loss=1.0431, t_f1r=[0.600, 0.838, 0.468, 0.637, 0.802, 0.563, 0.707], t_f1=0.659\n",
      "05/30 13:06:27 [INFO] eps=1, steps=4130/32524, lr=2.05e-05, t_loss=1.0331, t_f1r=[0.607, 0.840, 0.476, 0.643, 0.804, 0.567, 0.710], t_f1=0.664\n",
      "05/30 13:07:29 [INFO] eps=1, steps=4360/32524, lr=1.88e-05, t_loss=1.0237, t_f1r=[0.612, 0.843, 0.480, 0.647, 0.807, 0.573, 0.712], t_f1=0.668\n",
      "05/30 13:08:31 [INFO] eps=1, steps=4590/32524, lr=1.72e-05, t_loss=1.0142, t_f1r=[0.618, 0.846, 0.485, 0.651, 0.810, 0.577, 0.716], t_f1=0.672\n",
      "05/30 13:09:33 [INFO] eps=1, steps=4820/32524, lr=1.55e-05, t_loss=1.0058, t_f1r=[0.624, 0.849, 0.491, 0.655, 0.812, 0.581, 0.718], t_f1=0.675\n",
      "05/30 13:10:35 [INFO] eps=1, steps=5050/32524, lr=1.39e-05, t_loss=0.9974, t_f1r=[0.629, 0.852, 0.495, 0.658, 0.814, 0.587, 0.720], t_f1=0.679\n",
      "05/30 13:11:37 [INFO] eps=1, steps=5280/32524, lr=1.24e-05, t_loss=0.9886, t_f1r=[0.636, 0.854, 0.500, 0.662, 0.816, 0.591, 0.722], t_f1=0.683\n",
      "05/30 13:12:39 [INFO] eps=1, steps=5510/32524, lr=1.09e-05, t_loss=0.9818, t_f1r=[0.642, 0.857, 0.505, 0.665, 0.818, 0.595, 0.723], t_f1=0.686\n",
      "05/30 13:13:42 [INFO] eps=1, steps=5740/32524, lr=9.55e-06, t_loss=0.9738, t_f1r=[0.647, 0.860, 0.509, 0.667, 0.820, 0.600, 0.725], t_f1=0.690\n",
      "05/30 13:14:44 [INFO] eps=1, steps=5970/32524, lr=8.25e-06, t_loss=0.9664, t_f1r=[0.650, 0.863, 0.513, 0.671, 0.822, 0.603, 0.726], t_f1=0.693\n",
      "05/30 13:15:46 [INFO] eps=1, steps=6200/32524, lr=7.05e-06, t_loss=0.9581, t_f1r=[0.655, 0.866, 0.518, 0.674, 0.823, 0.608, 0.730], t_f1=0.696\n",
      "05/30 13:16:48 [INFO] eps=1, steps=6430/32524, lr=5.96e-06, t_loss=0.9507, t_f1r=[0.660, 0.868, 0.520, 0.678, 0.826, 0.613, 0.732], t_f1=0.699\n",
      "05/30 13:17:51 [INFO] eps=1, steps=6660/32524, lr=4.99e-06, t_loss=0.9440, t_f1r=[0.665, 0.871, 0.524, 0.680, 0.827, 0.616, 0.732], t_f1=0.702\n",
      "05/30 13:18:53 [INFO] eps=1, steps=6890/32524, lr=4.14e-06, t_loss=0.9369, t_f1r=[0.669, 0.873, 0.529, 0.683, 0.829, 0.620, 0.734], t_f1=0.705\n",
      "05/30 13:19:55 [INFO] eps=1, steps=7120/32524, lr=3.43e-06, t_loss=0.9304, t_f1r=[0.673, 0.875, 0.531, 0.685, 0.831, 0.624, 0.737], t_f1=0.708\n",
      "05/30 13:20:58 [INFO] eps=1, steps=7350/32524, lr=2.86e-06, t_loss=0.9244, t_f1r=[0.676, 0.877, 0.535, 0.687, 0.832, 0.628, 0.738], t_f1=0.710\n",
      "05/30 13:22:00 [INFO] eps=1, steps=7580/32524, lr=2.43e-06, t_loss=0.9176, t_f1r=[0.680, 0.879, 0.539, 0.690, 0.834, 0.632, 0.740], t_f1=0.713\n",
      "05/30 13:23:03 [INFO] eps=1, steps=7810/32524, lr=2.15e-06, t_loss=0.9116, t_f1r=[0.685, 0.881, 0.542, 0.693, 0.835, 0.635, 0.741], t_f1=0.716\n",
      "05/30 13:24:05 [INFO] eps=1, steps=8040/32524, lr=2.01e-06, t_loss=0.9062, t_f1r=[0.688, 0.883, 0.545, 0.694, 0.837, 0.638, 0.743], t_f1=0.718\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8853c67ae74abfb8c0280b34c2f1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 13:25:30 [INFO] steps=307/2376, v_loss=0.6666, v_f1r=[0.905, 0.000, 0.000, 0.000, 0.000, 0.000, 0.802], v_f1=0.244\n",
      "05/30 13:26:30 [INFO] steps=637/2376, v_loss=0.6096, v_f1r=[0.896, 0.962, 0.000, 0.000, 0.000, 0.000, 0.880], v_f1=0.391\n",
      "05/30 13:27:30 [INFO] steps=968/2376, v_loss=0.6756, v_f1r=[0.868, 0.956, 0.633, 0.000, 0.000, 0.793, 0.851], v_f1=0.586\n",
      "05/30 13:28:30 [INFO] steps=1299/2376, v_loss=0.6960, v_f1r=[0.828, 0.939, 0.589, 0.000, 0.000, 0.843, 0.830], v_f1=0.576\n",
      "05/30 13:29:31 [INFO] steps=1630/2376, v_loss=0.6581, v_f1r=[0.823, 0.938, 0.554, 0.000, 0.927, 0.845, 0.811], v_f1=0.700\n",
      "05/30 13:30:31 [INFO] steps=1961/2376, v_loss=0.6365, v_f1r=[0.821, 0.937, 0.516, 0.526, 0.936, 0.844, 0.792], v_f1=0.768\n",
      "05/30 13:31:31 [INFO] steps=2292/2376, v_loss=0.6560, v_f1r=[0.814, 0.935, 0.457, 0.811, 0.927, 0.837, 0.770], v_f1=0.793\n",
      "05/30 13:31:46 [INFO] eps=1, steps=8131/32524, lr=3.73e-05, t_loss=0.9036, v_f1r=[0.813, 0.934, 0.443, 0.823, 0.924, 0.835, 0.764], v_loss=0.6599*, v_f1=0.7909*\n",
      "05/30 13:31:48 [INFO] eps=1, steps=8140/32524, lr=3.73e-05, t_loss=0.7963, t_f1r=[0.645, 0.967, 0.619, 0.750, 0.857, 0.784, 0.759], t_f1=0.769\n",
      "05/30 13:32:50 [INFO] eps=1, steps=8370/32524, lr=3.73e-05, t_loss=0.8850, t_f1r=[0.713, 0.902, 0.527, 0.699, 0.830, 0.633, 0.733], t_f1=0.720\n",
      "05/30 13:33:51 [INFO] eps=1, steps=8600/32524, lr=3.71e-05, t_loss=0.8970, t_f1r=[0.699, 0.896, 0.528, 0.700, 0.828, 0.628, 0.740], t_f1=0.717\n",
      "05/30 13:34:52 [INFO] eps=1, steps=8830/32524, lr=3.67e-05, t_loss=0.8992, t_f1r=[0.695, 0.893, 0.537, 0.697, 0.827, 0.643, 0.738], t_f1=0.719\n",
      "05/30 13:35:54 [INFO] eps=1, steps=9060/32524, lr=3.62e-05, t_loss=0.8962, t_f1r=[0.695, 0.889, 0.540, 0.700, 0.831, 0.645, 0.738], t_f1=0.720\n",
      "05/30 13:36:55 [INFO] eps=1, steps=9290/32524, lr=3.56e-05, t_loss=0.8978, t_f1r=[0.697, 0.890, 0.540, 0.698, 0.829, 0.643, 0.741], t_f1=0.720\n",
      "05/30 13:37:56 [INFO] eps=1, steps=9520/32524, lr=3.49e-05, t_loss=0.8978, t_f1r=[0.699, 0.891, 0.542, 0.692, 0.830, 0.644, 0.738], t_f1=0.719\n",
      "05/30 13:38:58 [INFO] eps=1, steps=9750/32524, lr=3.4e-05, t_loss=0.8917, t_f1r=[0.704, 0.892, 0.547, 0.698, 0.830, 0.646, 0.741], t_f1=0.723\n",
      "05/30 13:40:00 [INFO] eps=1, steps=9980/32524, lr=3.3e-05, t_loss=0.8879, t_f1r=[0.702, 0.891, 0.550, 0.700, 0.834, 0.648, 0.746], t_f1=0.725\n",
      "05/30 13:41:01 [INFO] eps=1, steps=10210/32524, lr=3.19e-05, t_loss=0.8850, t_f1r=[0.706, 0.892, 0.551, 0.701, 0.835, 0.649, 0.747], t_f1=0.726\n",
      "05/30 13:42:03 [INFO] eps=1, steps=10440/32524, lr=3.08e-05, t_loss=0.8819, t_f1r=[0.708, 0.893, 0.554, 0.701, 0.835, 0.650, 0.748], t_f1=0.727\n",
      "05/30 13:43:05 [INFO] eps=1, steps=10670/32524, lr=2.95e-05, t_loss=0.8760, t_f1r=[0.710, 0.897, 0.559, 0.702, 0.838, 0.653, 0.751], t_f1=0.730\n",
      "05/30 13:44:06 [INFO] eps=1, steps=10900/32524, lr=2.82e-05, t_loss=0.8710, t_f1r=[0.711, 0.898, 0.562, 0.705, 0.840, 0.656, 0.752], t_f1=0.732\n",
      "05/30 13:45:08 [INFO] eps=1, steps=11130/32524, lr=2.67e-05, t_loss=0.8672, t_f1r=[0.713, 0.899, 0.563, 0.706, 0.843, 0.656, 0.753], t_f1=0.733\n",
      "05/30 13:46:10 [INFO] eps=1, steps=11360/32524, lr=2.53e-05, t_loss=0.8633, t_f1r=[0.715, 0.900, 0.566, 0.707, 0.844, 0.658, 0.754], t_f1=0.735\n",
      "05/30 13:47:12 [INFO] eps=1, steps=11590/32524, lr=2.38e-05, t_loss=0.8591, t_f1r=[0.719, 0.902, 0.569, 0.709, 0.845, 0.663, 0.755], t_f1=0.737\n",
      "05/30 13:48:14 [INFO] eps=1, steps=11820/32524, lr=2.22e-05, t_loss=0.8550, t_f1r=[0.722, 0.903, 0.572, 0.711, 0.845, 0.664, 0.757], t_f1=0.739\n",
      "05/30 13:49:15 [INFO] eps=1, steps=12050/32524, lr=2.07e-05, t_loss=0.8507, t_f1r=[0.726, 0.904, 0.574, 0.714, 0.847, 0.667, 0.758], t_f1=0.741\n",
      "05/30 13:50:17 [INFO] eps=1, steps=12280/32524, lr=1.91e-05, t_loss=0.8474, t_f1r=[0.728, 0.906, 0.577, 0.715, 0.847, 0.669, 0.759], t_f1=0.743\n",
      "05/30 13:51:20 [INFO] eps=1, steps=12510/32524, lr=1.75e-05, t_loss=0.8417, t_f1r=[0.731, 0.907, 0.580, 0.717, 0.849, 0.672, 0.761], t_f1=0.745\n",
      "05/30 13:52:22 [INFO] eps=1, steps=12740/32524, lr=1.6e-05, t_loss=0.8363, t_f1r=[0.734, 0.909, 0.584, 0.720, 0.851, 0.676, 0.762], t_f1=0.748\n",
      "05/30 13:53:24 [INFO] eps=1, steps=12970/32524, lr=1.45e-05, t_loss=0.8327, t_f1r=[0.736, 0.910, 0.587, 0.721, 0.853, 0.679, 0.763], t_f1=0.750\n",
      "05/30 13:54:26 [INFO] eps=1, steps=13200/32524, lr=1.3e-05, t_loss=0.8289, t_f1r=[0.738, 0.912, 0.589, 0.722, 0.854, 0.683, 0.764], t_f1=0.752\n",
      "05/30 13:55:28 [INFO] eps=1, steps=13430/32524, lr=1.16e-05, t_loss=0.8258, t_f1r=[0.741, 0.913, 0.592, 0.722, 0.855, 0.685, 0.764], t_f1=0.753\n",
      "05/30 13:56:30 [INFO] eps=1, steps=13660/32524, lr=1.02e-05, t_loss=0.8213, t_f1r=[0.744, 0.913, 0.594, 0.725, 0.856, 0.688, 0.765], t_f1=0.755\n",
      "05/30 13:57:32 [INFO] eps=1, steps=13890/32524, lr=8.92e-06, t_loss=0.8181, t_f1r=[0.748, 0.914, 0.596, 0.727, 0.856, 0.690, 0.765], t_f1=0.757\n",
      "05/30 13:58:34 [INFO] eps=1, steps=14120/32524, lr=7.71e-06, t_loss=0.8136, t_f1r=[0.750, 0.916, 0.597, 0.729, 0.858, 0.694, 0.767], t_f1=0.759\n",
      "05/30 13:59:37 [INFO] eps=1, steps=14350/32524, lr=6.61e-06, t_loss=0.8092, t_f1r=[0.752, 0.917, 0.600, 0.731, 0.860, 0.696, 0.768], t_f1=0.761\n",
      "05/30 14:00:39 [INFO] eps=1, steps=14580/32524, lr=5.6e-06, t_loss=0.8053, t_f1r=[0.755, 0.918, 0.602, 0.732, 0.861, 0.700, 0.767], t_f1=0.762\n",
      "05/30 14:01:41 [INFO] eps=1, steps=14810/32524, lr=4.71e-06, t_loss=0.8010, t_f1r=[0.757, 0.920, 0.605, 0.734, 0.862, 0.703, 0.769], t_f1=0.764\n",
      "05/30 14:02:44 [INFO] eps=1, steps=15040/32524, lr=3.93e-06, t_loss=0.7968, t_f1r=[0.760, 0.921, 0.608, 0.736, 0.863, 0.704, 0.770], t_f1=0.766\n",
      "05/30 14:03:46 [INFO] eps=1, steps=15270/32524, lr=3.28e-06, t_loss=0.7924, t_f1r=[0.762, 0.922, 0.611, 0.738, 0.864, 0.708, 0.771], t_f1=0.768\n",
      "05/30 14:04:49 [INFO] eps=1, steps=15500/32524, lr=2.76e-06, t_loss=0.7879, t_f1r=[0.765, 0.923, 0.615, 0.740, 0.865, 0.710, 0.772], t_f1=0.770\n",
      "05/30 14:05:51 [INFO] eps=1, steps=15730/32524, lr=2.37e-06, t_loss=0.7846, t_f1r=[0.768, 0.924, 0.616, 0.741, 0.866, 0.713, 0.773], t_f1=0.772\n",
      "05/30 14:06:54 [INFO] eps=1, steps=15960/32524, lr=2.12e-06, t_loss=0.7807, t_f1r=[0.770, 0.925, 0.619, 0.742, 0.867, 0.716, 0.774], t_f1=0.773\n",
      "05/30 14:07:56 [INFO] eps=1, steps=16190/32524, lr=2.01e-06, t_loss=0.7772, t_f1r=[0.773, 0.926, 0.622, 0.744, 0.868, 0.717, 0.775], t_f1=0.775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef908a190b649ed87081ffc1771b735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f99dfc4c680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1665, in __del__\n",
      "    Exception ignored in: Exception ignored in: self._shutdown_workers()<function _MultiProcessingDataLoaderIter.__del__ at 0x7f99dfc4c680>\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7f99dfc4c680>\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1665, in __del__\n",
      "Traceback (most recent call last):\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1648, in _shutdown_workers\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1665, in __del__\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1648, in _shutdown_workers\n",
      "        if w.is_alive():if w.is_alive():\n",
      " \n",
      "    self._shutdown_workers() \n",
      "   File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1648, in _shutdown_workers\n",
      "         if w.is_alive():\n",
      "   ^ ^  ^^^^^  ^^^^^  ^ ^^^ ^ ^^^^^^^^^^\n",
      "\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^    assert self._parent_pid == os.getpid(), 'can only test a child process'  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "^\n",
      "     ^^assert self._parent_pid == os.getpid(), 'can only test a child process' ^ ^  ^ ^ ^ \n",
      "\n",
      "   File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "      ^assert self._parent_pid == os.getpid(), 'can only test a child process'^^ \n",
      "^  ^   ^ ^  ^   ^  ^^  ^  ^ ^  ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^AssertionError^^: ^^^can only test a child process^\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^AssertionError: \n",
      "can only test a child processAssertionError\n",
      ": can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f99dfc4c680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1665, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1648, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f99dfc4c680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1665, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1648, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f99dfc4c680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1665, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1648, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f99dfc4c680>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1665, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1648, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/home/notebook/miniconda3/envs/py312/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 14:09:16 [INFO] steps=324/2376, v_loss=0.5911, v_f1r=[0.929, 0.000, 0.000, 0.000, 0.000, 0.000, 0.845], v_f1=0.253\n",
      "05/30 14:10:16 [INFO] steps=655/2376, v_loss=0.5674, v_f1r=[0.920, 0.968, 0.000, 0.000, 0.000, 0.000, 0.877], v_f1=0.395\n",
      "05/30 14:11:16 [INFO] steps=986/2376, v_loss=0.6336, v_f1r=[0.889, 0.961, 0.684, 0.000, 0.000, 0.826, 0.851], v_f1=0.601\n",
      "05/30 14:12:16 [INFO] steps=1317/2376, v_loss=0.6495, v_f1r=[0.854, 0.948, 0.642, 0.000, 0.000, 0.865, 0.832], v_f1=0.592\n",
      "05/30 14:13:16 [INFO] steps=1648/2376, v_loss=0.6152, v_f1r=[0.852, 0.947, 0.603, 0.000, 0.935, 0.865, 0.819], v_f1=0.717\n",
      "05/30 14:14:17 [INFO] steps=1978/2376, v_loss=0.5990, v_f1r=[0.850, 0.947, 0.560, 0.610, 0.943, 0.864, 0.805], v_f1=0.797\n",
      "05/30 14:15:17 [INFO] steps=2308/2376, v_loss=0.6120, v_f1r=[0.844, 0.945, 0.498, 0.835, 0.937, 0.858, 0.788], v_f1=0.815\n",
      "05/30 14:15:29 [INFO] eps=1, steps=16262/32524, lr=3.49e-05, t_loss=0.7757, v_f1r=[0.842, 0.944, 0.487, 0.843, 0.935, 0.856, 0.784], v_loss=0.6146*, v_f1=0.8132*\n",
      "05/30 14:15:31 [INFO] eps=1, steps=16270/32524, lr=3.49e-05, t_loss=0.6463, t_f1r=[0.882, 0.971, 0.667, 0.773, 0.926, 0.667, 0.759], t_f1=0.806\n",
      "05/30 14:16:39 [INFO] eps=1, steps=16450/32524, lr=3.48e-05, t_loss=0.7997, t_f1r=[0.765, 0.927, 0.608, 0.739, 0.864, 0.683, 0.754], t_f1=0.763\n",
      "05/30 14:17:41 [INFO] eps=1, steps=16680/32524, lr=3.47e-05, t_loss=0.8118, t_f1r=[0.755, 0.916, 0.606, 0.740, 0.853, 0.701, 0.768], t_f1=0.763\n",
      "05/30 14:18:42 [INFO] eps=1, steps=16910/32524, lr=3.44e-05, t_loss=0.8079, t_f1r=[0.752, 0.916, 0.617, 0.738, 0.857, 0.692, 0.763], t_f1=0.762\n",
      "05/30 14:19:43 [INFO] eps=1, steps=17140/32524, lr=3.39e-05, t_loss=0.8058, t_f1r=[0.754, 0.914, 0.612, 0.740, 0.858, 0.690, 0.764], t_f1=0.762\n",
      "05/30 14:20:45 [INFO] eps=1, steps=17370/32524, lr=3.34e-05, t_loss=0.8062, t_f1r=[0.753, 0.914, 0.615, 0.734, 0.859, 0.695, 0.761], t_f1=0.762\n",
      "05/30 14:21:46 [INFO] eps=1, steps=17600/32524, lr=3.27e-05, t_loss=0.8069, t_f1r=[0.750, 0.913, 0.614, 0.734, 0.860, 0.697, 0.759], t_f1=0.761\n",
      "05/30 14:22:48 [INFO] eps=1, steps=17830/32524, lr=3.19e-05, t_loss=0.8045, t_f1r=[0.750, 0.914, 0.616, 0.733, 0.861, 0.699, 0.758], t_f1=0.761\n",
      "05/30 14:23:50 [INFO] eps=1, steps=18060/32524, lr=3.11e-05, t_loss=0.8042, t_f1r=[0.751, 0.913, 0.617, 0.734, 0.863, 0.699, 0.759], t_f1=0.762\n",
      "05/30 14:24:51 [INFO] eps=1, steps=18290/32524, lr=3.01e-05, t_loss=0.8027, t_f1r=[0.749, 0.913, 0.622, 0.736, 0.866, 0.698, 0.761], t_f1=0.764\n",
      "05/30 14:25:53 [INFO] eps=1, steps=18520/32524, lr=2.9e-05, t_loss=0.8023, t_f1r=[0.751, 0.912, 0.626, 0.737, 0.868, 0.698, 0.762], t_f1=0.765\n",
      "05/30 14:26:54 [INFO] eps=1, steps=18750/32524, lr=2.78e-05, t_loss=0.8003, t_f1r=[0.753, 0.913, 0.627, 0.736, 0.866, 0.701, 0.763], t_f1=0.766\n",
      "05/30 14:27:56 [INFO] eps=1, steps=18980/32524, lr=2.66e-05, t_loss=0.7980, t_f1r=[0.757, 0.914, 0.628, 0.737, 0.868, 0.702, 0.765], t_f1=0.767\n",
      "05/30 14:28:58 [INFO] eps=1, steps=19210/32524, lr=2.53e-05, t_loss=0.7967, t_f1r=[0.758, 0.914, 0.630, 0.737, 0.868, 0.701, 0.766], t_f1=0.768\n",
      "05/30 14:30:00 [INFO] eps=1, steps=19440/32524, lr=2.4e-05, t_loss=0.7919, t_f1r=[0.762, 0.915, 0.632, 0.740, 0.869, 0.704, 0.769], t_f1=0.770\n",
      "05/30 14:31:02 [INFO] eps=1, steps=19670/32524, lr=2.26e-05, t_loss=0.7895, t_f1r=[0.764, 0.916, 0.634, 0.740, 0.870, 0.705, 0.770], t_f1=0.771\n",
      "05/30 14:32:04 [INFO] eps=1, steps=19900/32524, lr=2.11e-05, t_loss=0.7864, t_f1r=[0.765, 0.917, 0.635, 0.739, 0.870, 0.707, 0.770], t_f1=0.772\n",
      "05/30 14:33:06 [INFO] eps=1, steps=20130/32524, lr=1.97e-05, t_loss=0.7838, t_f1r=[0.767, 0.918, 0.635, 0.741, 0.871, 0.708, 0.772], t_f1=0.773\n",
      "05/30 14:34:08 [INFO] eps=1, steps=20360/32524, lr=1.82e-05, t_loss=0.7803, t_f1r=[0.768, 0.919, 0.635, 0.742, 0.870, 0.710, 0.772], t_f1=0.774\n",
      "05/30 14:35:10 [INFO] eps=1, steps=20590/32524, lr=1.68e-05, t_loss=0.7775, t_f1r=[0.771, 0.920, 0.636, 0.743, 0.871, 0.712, 0.773], t_f1=0.775\n",
      "05/30 14:36:12 [INFO] eps=1, steps=20820/32524, lr=1.53e-05, t_loss=0.7744, t_f1r=[0.774, 0.921, 0.638, 0.743, 0.871, 0.713, 0.774], t_f1=0.776\n",
      "05/30 14:37:14 [INFO] eps=1, steps=21050/32524, lr=1.39e-05, t_loss=0.7704, t_f1r=[0.776, 0.922, 0.642, 0.745, 0.872, 0.714, 0.776], t_f1=0.778\n",
      "05/30 14:38:16 [INFO] eps=1, steps=21280/32524, lr=1.25e-05, t_loss=0.7663, t_f1r=[0.779, 0.923, 0.644, 0.747, 0.873, 0.716, 0.778], t_f1=0.780\n",
      "05/30 14:39:18 [INFO] eps=1, steps=21510/32524, lr=1.12e-05, t_loss=0.7626, t_f1r=[0.781, 0.924, 0.647, 0.749, 0.874, 0.720, 0.779], t_f1=0.782\n",
      "05/30 14:40:21 [INFO] eps=1, steps=21740/32524, lr=9.9e-06, t_loss=0.7595, t_f1r=[0.783, 0.925, 0.647, 0.750, 0.875, 0.721, 0.781], t_f1=0.783\n",
      "05/30 14:41:23 [INFO] eps=1, steps=21970/32524, lr=8.69e-06, t_loss=0.7565, t_f1r=[0.784, 0.926, 0.650, 0.751, 0.876, 0.724, 0.782], t_f1=0.785\n",
      "05/30 14:42:25 [INFO] eps=1, steps=22200/32524, lr=7.55e-06, t_loss=0.7535, t_f1r=[0.786, 0.927, 0.651, 0.753, 0.877, 0.725, 0.783], t_f1=0.786\n",
      "05/30 14:43:27 [INFO] eps=1, steps=22430/32524, lr=6.5e-06, t_loss=0.7499, t_f1r=[0.788, 0.928, 0.654, 0.754, 0.877, 0.728, 0.784], t_f1=0.787\n",
      "05/30 14:44:30 [INFO] eps=1, steps=22660/32524, lr=5.55e-06, t_loss=0.7465, t_f1r=[0.790, 0.928, 0.656, 0.756, 0.877, 0.730, 0.785], t_f1=0.789\n",
      "05/30 14:45:32 [INFO] eps=1, steps=22890/32524, lr=4.69e-06, t_loss=0.7428, t_f1r=[0.792, 0.929, 0.658, 0.758, 0.879, 0.732, 0.786], t_f1=0.790\n",
      "05/30 14:46:35 [INFO] eps=1, steps=23120/32524, lr=3.95e-06, t_loss=0.7396, t_f1r=[0.794, 0.930, 0.661, 0.760, 0.879, 0.734, 0.787], t_f1=0.792\n",
      "05/30 14:47:37 [INFO] eps=1, steps=23350/32524, lr=3.32e-06, t_loss=0.7360, t_f1r=[0.795, 0.931, 0.662, 0.762, 0.880, 0.736, 0.788], t_f1=0.793\n",
      "05/30 14:48:40 [INFO] eps=1, steps=23580/32524, lr=2.8e-06, t_loss=0.7325, t_f1r=[0.798, 0.932, 0.665, 0.763, 0.880, 0.739, 0.789], t_f1=0.795\n",
      "05/30 14:49:42 [INFO] eps=1, steps=23810/32524, lr=2.42e-06, t_loss=0.7290, t_f1r=[0.800, 0.933, 0.667, 0.765, 0.881, 0.740, 0.790], t_f1=0.797\n",
      "05/30 14:50:45 [INFO] eps=1, steps=24040/32524, lr=2.15e-06, t_loss=0.7255, t_f1r=[0.802, 0.934, 0.669, 0.766, 0.882, 0.742, 0.792], t_f1=0.798\n",
      "05/30 14:51:47 [INFO] eps=1, steps=24270/32524, lr=2.02e-06, t_loss=0.7227, t_f1r=[0.804, 0.935, 0.671, 0.767, 0.882, 0.745, 0.792], t_f1=0.799\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0c3061c9d8475fb4300290107f1a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 14:53:21 [INFO] steps=325/2376, v_loss=0.5870, v_f1r=[0.931, 0.000, 0.000, 0.000, 0.000, 0.000, 0.853], v_f1=0.255\n",
      "05/30 14:54:21 [INFO] steps=657/2376, v_loss=0.5597, v_f1r=[0.922, 0.972, 0.000, 0.000, 0.000, 0.000, 0.878], v_f1=0.396\n",
      "05/30 14:55:21 [INFO] steps=988/2376, v_loss=0.6164, v_f1r=[0.898, 0.965, 0.695, 0.000, 0.000, 0.845, 0.854], v_f1=0.608\n",
      "05/30 14:56:21 [INFO] steps=1318/2376, v_loss=0.6312, v_f1r=[0.871, 0.954, 0.650, 0.000, 0.000, 0.877, 0.837], v_f1=0.598\n",
      "05/30 14:57:22 [INFO] steps=1649/2376, v_loss=0.5945, v_f1r=[0.869, 0.953, 0.619, 0.000, 0.937, 0.878, 0.824], v_f1=0.726\n",
      "05/30 14:58:22 [INFO] steps=1979/2376, v_loss=0.5765, v_f1r=[0.868, 0.953, 0.578, 0.618, 0.946, 0.877, 0.811], v_f1=0.807\n",
      "05/30 14:59:22 [INFO] steps=2309/2376, v_loss=0.5855, v_f1r=[0.864, 0.951, 0.515, 0.845, 0.940, 0.871, 0.796], v_f1=0.826\n",
      "05/30 14:59:34 [INFO] eps=1, steps=24393/32524, lr=3.26e-05, t_loss=0.7210, v_f1r=[0.863, 0.951, 0.505, 0.854, 0.939, 0.870, 0.793], v_loss=0.5873*, v_f1=0.8250*\n",
      "05/30 14:59:36 [INFO] eps=1, steps=24400/32524, lr=3.26e-05, t_loss=0.5158, t_f1r=[0.842, 0.979, 0.706, 0.933, 0.947, 0.783, 0.839], t_f1=0.861\n",
      "05/30 15:00:37 [INFO] eps=1, steps=24630/32524, lr=3.25e-05, t_loss=0.7403, t_f1r=[0.775, 0.912, 0.643, 0.772, 0.880, 0.735, 0.781], t_f1=0.785\n",
      "05/30 15:01:37 [INFO] eps=1, steps=24780/32524, lr=3.24e-05, t_loss=0.7420, t_f1r=[0.777, 0.916, 0.648, 0.762, 0.885, 0.742, 0.781], t_f1=0.787\n",
      "05/30 15:02:38 [INFO] eps=1, steps=25010/32524, lr=3.21e-05, t_loss=0.7385, t_f1r=[0.779, 0.923, 0.656, 0.767, 0.888, 0.734, 0.779], t_f1=0.789\n",
      "05/30 15:03:40 [INFO] eps=1, steps=25240/32524, lr=3.18e-05, t_loss=0.7451, t_f1r=[0.785, 0.923, 0.650, 0.757, 0.884, 0.737, 0.781], t_f1=0.788\n",
      "05/30 15:04:41 [INFO] eps=1, steps=25470/32524, lr=3.13e-05, t_loss=0.7485, t_f1r=[0.783, 0.921, 0.650, 0.759, 0.881, 0.732, 0.780], t_f1=0.787\n",
      "05/30 15:05:43 [INFO] eps=1, steps=25700/32524, lr=3.07e-05, t_loss=0.7483, t_f1r=[0.789, 0.923, 0.651, 0.756, 0.879, 0.735, 0.782], t_f1=0.788\n",
      "05/30 15:06:44 [INFO] eps=1, steps=25930/32524, lr=2.99e-05, t_loss=0.7488, t_f1r=[0.790, 0.923, 0.650, 0.754, 0.878, 0.736, 0.782], t_f1=0.788\n",
      "05/30 15:07:46 [INFO] eps=1, steps=26160/32524, lr=2.91e-05, t_loss=0.7467, t_f1r=[0.791, 0.924, 0.652, 0.756, 0.877, 0.736, 0.785], t_f1=0.789\n",
      "05/30 15:08:48 [INFO] eps=1, steps=26390/32524, lr=2.82e-05, t_loss=0.7471, t_f1r=[0.792, 0.923, 0.651, 0.756, 0.879, 0.737, 0.783], t_f1=0.789\n",
      "05/30 15:09:49 [INFO] eps=1, steps=26620/32524, lr=2.72e-05, t_loss=0.7460, t_f1r=[0.791, 0.922, 0.653, 0.758, 0.880, 0.737, 0.784], t_f1=0.789\n",
      "05/30 15:10:51 [INFO] eps=1, steps=26850/32524, lr=2.62e-05, t_loss=0.7449, t_f1r=[0.793, 0.923, 0.653, 0.757, 0.880, 0.738, 0.784], t_f1=0.790\n",
      "05/30 15:11:53 [INFO] eps=1, steps=27080/32524, lr=2.5e-05, t_loss=0.7437, t_f1r=[0.794, 0.924, 0.656, 0.759, 0.879, 0.738, 0.785], t_f1=0.791\n",
      "05/30 15:12:55 [INFO] eps=1, steps=27310/32524, lr=2.38e-05, t_loss=0.7432, t_f1r=[0.794, 0.925, 0.657, 0.758, 0.879, 0.739, 0.785], t_f1=0.791\n",
      "05/30 15:13:57 [INFO] eps=1, steps=27540/32524, lr=2.26e-05, t_loss=0.7427, t_f1r=[0.794, 0.926, 0.656, 0.758, 0.878, 0.739, 0.786], t_f1=0.791\n",
      "05/30 15:14:59 [INFO] eps=1, steps=27770/32524, lr=2.13e-05, t_loss=0.7392, t_f1r=[0.796, 0.927, 0.659, 0.758, 0.878, 0.742, 0.787], t_f1=0.792\n",
      "05/30 15:16:00 [INFO] eps=1, steps=28000/32524, lr=2e-05, t_loss=0.7360, t_f1r=[0.799, 0.928, 0.661, 0.759, 0.879, 0.742, 0.787], t_f1=0.794\n",
      "05/30 15:17:02 [INFO] eps=1, steps=28230/32524, lr=1.86e-05, t_loss=0.7335, t_f1r=[0.800, 0.929, 0.663, 0.761, 0.880, 0.743, 0.788], t_f1=0.795\n",
      "05/30 15:18:04 [INFO] eps=1, steps=28460/32524, lr=1.73e-05, t_loss=0.7300, t_f1r=[0.801, 0.929, 0.665, 0.763, 0.881, 0.745, 0.790], t_f1=0.796\n",
      "05/30 15:19:07 [INFO] eps=1, steps=28690/32524, lr=1.59e-05, t_loss=0.7265, t_f1r=[0.804, 0.930, 0.667, 0.765, 0.881, 0.747, 0.790], t_f1=0.798\n",
      "05/30 15:20:09 [INFO] eps=1, steps=28920/32524, lr=1.46e-05, t_loss=0.7236, t_f1r=[0.806, 0.931, 0.668, 0.766, 0.882, 0.749, 0.791], t_f1=0.799\n",
      "05/30 15:21:11 [INFO] eps=1, steps=29150/32524, lr=1.32e-05, t_loss=0.7206, t_f1r=[0.807, 0.933, 0.670, 0.767, 0.883, 0.750, 0.790], t_f1=0.800\n",
      "05/30 15:22:13 [INFO] eps=1, steps=29380/32524, lr=1.2e-05, t_loss=0.7173, t_f1r=[0.809, 0.934, 0.672, 0.769, 0.883, 0.752, 0.792], t_f1=0.802\n",
      "05/30 15:23:15 [INFO] eps=1, steps=29610/32524, lr=1.07e-05, t_loss=0.7141, t_f1r=[0.811, 0.935, 0.674, 0.771, 0.884, 0.754, 0.792], t_f1=0.803\n",
      "05/30 15:24:17 [INFO] eps=1, steps=29840/32524, lr=9.51e-06, t_loss=0.7105, t_f1r=[0.813, 0.936, 0.676, 0.774, 0.885, 0.757, 0.794], t_f1=0.805\n",
      "05/30 15:25:19 [INFO] eps=1, steps=30070/32524, lr=8.37e-06, t_loss=0.7074, t_f1r=[0.814, 0.937, 0.678, 0.775, 0.886, 0.759, 0.795], t_f1=0.806\n",
      "05/30 15:26:22 [INFO] eps=1, steps=30300/32524, lr=7.3e-06, t_loss=0.7040, t_f1r=[0.816, 0.938, 0.680, 0.776, 0.887, 0.761, 0.796], t_f1=0.808\n",
      "05/30 15:27:24 [INFO] eps=1, steps=30530/32524, lr=6.32e-06, t_loss=0.7011, t_f1r=[0.818, 0.938, 0.683, 0.777, 0.887, 0.763, 0.795], t_f1=0.809\n",
      "05/30 15:28:27 [INFO] eps=1, steps=30760/32524, lr=5.41e-06, t_loss=0.6980, t_f1r=[0.819, 0.939, 0.686, 0.778, 0.888, 0.765, 0.796], t_f1=0.810\n",
      "05/30 15:29:29 [INFO] eps=1, steps=30990/32524, lr=4.61e-06, t_loss=0.6948, t_f1r=[0.821, 0.939, 0.688, 0.780, 0.889, 0.767, 0.797], t_f1=0.812\n",
      "05/30 15:30:31 [INFO] eps=1, steps=31220/32524, lr=3.9e-06, t_loss=0.6917, t_f1r=[0.824, 0.940, 0.691, 0.782, 0.889, 0.768, 0.798], t_f1=0.813\n",
      "05/30 15:31:34 [INFO] eps=1, steps=31450/32524, lr=3.3e-06, t_loss=0.6888, t_f1r=[0.826, 0.941, 0.693, 0.783, 0.890, 0.770, 0.799], t_f1=0.815\n",
      "05/30 15:32:36 [INFO] eps=1, steps=31680/32524, lr=2.81e-06, t_loss=0.6855, t_f1r=[0.827, 0.942, 0.696, 0.785, 0.891, 0.772, 0.800], t_f1=0.816\n",
      "05/30 15:33:39 [INFO] eps=1, steps=31910/32524, lr=2.43e-06, t_loss=0.6834, t_f1r=[0.828, 0.942, 0.697, 0.786, 0.892, 0.774, 0.801], t_f1=0.817\n",
      "05/30 15:34:41 [INFO] eps=1, steps=32140/32524, lr=2.17e-06, t_loss=0.6809, t_f1r=[0.830, 0.943, 0.698, 0.787, 0.892, 0.776, 0.802], t_f1=0.818\n",
      "05/30 15:35:44 [INFO] eps=1, steps=32370/32524, lr=2.03e-06, t_loss=0.6780, t_f1r=[0.832, 0.944, 0.701, 0.788, 0.892, 0.777, 0.803], t_f1=0.820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0530 15:36:46.757000 128577 site-packages/torch/_inductor/utils.py:1408] [0/2] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4cfda34f7b463cac89f44fe5e53f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 15:38:09 [INFO] steps=324/2376, v_loss=0.5450, v_f1r=[0.939, 0.000, 0.000, 0.000, 0.000, 0.000, 0.853], v_f1=0.256\n",
      "05/30 15:39:09 [INFO] steps=655/2376, v_loss=0.5293, v_f1r=[0.931, 0.973, 0.000, 0.000, 0.000, 0.000, 0.888], v_f1=0.399\n",
      "05/30 15:40:09 [INFO] steps=986/2376, v_loss=0.5872, v_f1r=[0.910, 0.969, 0.703, 0.000, 0.000, 0.856, 0.861], v_f1=0.614\n",
      "05/30 15:41:09 [INFO] steps=1316/2376, v_loss=0.6002, v_f1r=[0.882, 0.961, 0.657, 0.000, 0.000, 0.886, 0.841], v_f1=0.604\n",
      "05/30 15:42:09 [INFO] steps=1647/2376, v_loss=0.5721, v_f1r=[0.880, 0.961, 0.619, 0.000, 0.939, 0.886, 0.828], v_f1=0.730\n",
      "05/30 15:43:09 [INFO] steps=1976/2376, v_loss=0.5585, v_f1r=[0.879, 0.960, 0.579, 0.646, 0.946, 0.885, 0.814], v_f1=0.816\n",
      "05/30 15:44:09 [INFO] steps=2306/2376, v_loss=0.5667, v_f1r=[0.875, 0.959, 0.528, 0.860, 0.942, 0.879, 0.797], v_f1=0.834\n",
      "05/30 15:44:22 [INFO] eps=1, steps=32524/32524, lr=3.04e-05, t_loss=0.6764, v_f1r=[0.874, 0.958, 0.518, 0.868, 0.941, 0.878, 0.793], v_loss=0.5680*, v_f1=0.8329*\n",
      "05/30 15:44:22 [INFO] already max rock_type : 4\n",
      "05/30 15:44:22 [INFO] load_img_size=448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9cecaf00a847ba8e0d3e4ec58a6d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/32524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 15:45:24 [INFO] eps=2, steps=150/32524, lr=3.04e-05, t_loss=0.6968, t_f1r=[0.814, 0.940, 0.724, 0.779, 0.902, 0.764, 0.798], t_f1=0.817\n",
      "05/30 15:46:26 [INFO] eps=2, steps=380/32524, lr=3.03e-05, t_loss=0.6933, t_f1r=[0.835, 0.946, 0.716, 0.779, 0.889, 0.767, 0.790], t_f1=0.817\n",
      "05/30 15:47:27 [INFO] eps=2, steps=610/32524, lr=3e-05, t_loss=0.7065, t_f1r=[0.822, 0.941, 0.705, 0.770, 0.883, 0.773, 0.791], t_f1=0.812\n",
      "05/30 15:48:28 [INFO] eps=2, steps=840/32524, lr=2.97e-05, t_loss=0.7075, t_f1r=[0.820, 0.941, 0.691, 0.777, 0.886, 0.771, 0.786], t_f1=0.810\n",
      "05/30 15:49:29 [INFO] eps=2, steps=1070/32524, lr=2.92e-05, t_loss=0.7042, t_f1r=[0.819, 0.943, 0.693, 0.778, 0.884, 0.770, 0.794], t_f1=0.812\n",
      "05/30 15:50:31 [INFO] eps=2, steps=1300/32524, lr=2.87e-05, t_loss=0.7049, t_f1r=[0.815, 0.943, 0.689, 0.777, 0.885, 0.767, 0.794], t_f1=0.810\n",
      "05/30 15:51:32 [INFO] eps=2, steps=1530/32524, lr=2.8e-05, t_loss=0.7004, t_f1r=[0.821, 0.942, 0.691, 0.781, 0.883, 0.772, 0.798], t_f1=0.812\n",
      "05/30 15:52:34 [INFO] eps=2, steps=1760/32524, lr=2.73e-05, t_loss=0.7027, t_f1r=[0.818, 0.940, 0.691, 0.781, 0.882, 0.769, 0.796], t_f1=0.811\n",
      "05/30 15:53:35 [INFO] eps=2, steps=1990/32524, lr=2.64e-05, t_loss=0.7044, t_f1r=[0.819, 0.940, 0.689, 0.778, 0.882, 0.767, 0.795], t_f1=0.810\n",
      "05/30 15:54:37 [INFO] eps=2, steps=2220/32524, lr=2.55e-05, t_loss=0.7035, t_f1r=[0.819, 0.940, 0.688, 0.777, 0.884, 0.769, 0.796], t_f1=0.810\n",
      "05/30 15:55:39 [INFO] eps=2, steps=2450/32524, lr=2.45e-05, t_loss=0.7009, t_f1r=[0.819, 0.940, 0.690, 0.778, 0.885, 0.768, 0.796], t_f1=0.811\n",
      "05/30 15:56:40 [INFO] eps=2, steps=2680/32524, lr=2.35e-05, t_loss=0.6992, t_f1r=[0.820, 0.940, 0.690, 0.779, 0.885, 0.770, 0.796], t_f1=0.812\n",
      "05/30 15:57:42 [INFO] eps=2, steps=2910/32524, lr=2.23e-05, t_loss=0.6968, t_f1r=[0.822, 0.940, 0.694, 0.779, 0.885, 0.770, 0.798], t_f1=0.813\n",
      "05/30 15:58:44 [INFO] eps=2, steps=3140/32524, lr=2.12e-05, t_loss=0.6944, t_f1r=[0.823, 0.940, 0.698, 0.779, 0.886, 0.771, 0.799], t_f1=0.814\n",
      "05/30 15:59:46 [INFO] eps=2, steps=3370/32524, lr=2e-05, t_loss=0.6933, t_f1r=[0.824, 0.940, 0.700, 0.779, 0.887, 0.769, 0.801], t_f1=0.814\n",
      "05/30 16:00:47 [INFO] eps=2, steps=3600/32524, lr=1.88e-05, t_loss=0.6913, t_f1r=[0.824, 0.941, 0.700, 0.781, 0.887, 0.771, 0.800], t_f1=0.815\n",
      "05/30 16:01:49 [INFO] eps=2, steps=3830/32524, lr=1.75e-05, t_loss=0.6896, t_f1r=[0.825, 0.942, 0.701, 0.782, 0.887, 0.772, 0.800], t_f1=0.816\n",
      "05/30 16:02:50 [INFO] eps=2, steps=3980/32524, lr=1.67e-05, t_loss=0.6885, t_f1r=[0.826, 0.942, 0.702, 0.782, 0.887, 0.773, 0.799], t_f1=0.816\n",
      "05/30 16:03:52 [INFO] eps=2, steps=4210/32524, lr=1.54e-05, t_loss=0.6855, t_f1r=[0.827, 0.942, 0.704, 0.783, 0.888, 0.774, 0.801], t_f1=0.817\n",
      "05/30 16:04:55 [INFO] eps=2, steps=4440/32524, lr=1.42e-05, t_loss=0.6827, t_f1r=[0.829, 0.943, 0.707, 0.785, 0.889, 0.775, 0.802], t_f1=0.818\n",
      "05/30 16:05:57 [INFO] eps=2, steps=4670/32524, lr=1.29e-05, t_loss=0.6795, t_f1r=[0.830, 0.944, 0.708, 0.786, 0.890, 0.778, 0.804], t_f1=0.820\n",
      "05/30 16:06:59 [INFO] eps=2, steps=4900/32524, lr=1.17e-05, t_loss=0.6768, t_f1r=[0.831, 0.944, 0.711, 0.787, 0.890, 0.779, 0.805], t_f1=0.821\n",
      "05/30 16:08:02 [INFO] eps=2, steps=5130/32524, lr=1.05e-05, t_loss=0.6747, t_f1r=[0.832, 0.945, 0.713, 0.786, 0.891, 0.780, 0.805], t_f1=0.822\n",
      "05/30 16:09:04 [INFO] eps=2, steps=5360/32524, lr=9.4e-06, t_loss=0.6727, t_f1r=[0.833, 0.945, 0.715, 0.787, 0.891, 0.781, 0.806], t_f1=0.823\n",
      "05/30 16:10:07 [INFO] eps=2, steps=5590/32524, lr=8.32e-06, t_loss=0.6696, t_f1r=[0.834, 0.946, 0.717, 0.790, 0.892, 0.783, 0.807], t_f1=0.824\n",
      "05/30 16:11:09 [INFO] eps=2, steps=5820/32524, lr=7.3e-06, t_loss=0.6664, t_f1r=[0.836, 0.948, 0.718, 0.791, 0.893, 0.785, 0.808], t_f1=0.825\n",
      "05/30 16:12:12 [INFO] eps=2, steps=6050/32524, lr=6.35e-06, t_loss=0.6633, t_f1r=[0.836, 0.948, 0.720, 0.792, 0.894, 0.787, 0.809], t_f1=0.827\n",
      "05/30 16:13:14 [INFO] eps=2, steps=6280/32524, lr=5.48e-06, t_loss=0.6605, t_f1r=[0.838, 0.949, 0.723, 0.795, 0.894, 0.788, 0.810], t_f1=0.828\n",
      "05/30 16:14:17 [INFO] eps=2, steps=6510/32524, lr=4.7e-06, t_loss=0.6580, t_f1r=[0.839, 0.950, 0.725, 0.795, 0.894, 0.789, 0.811], t_f1=0.829\n",
      "05/30 16:15:20 [INFO] eps=2, steps=6740/32524, lr=4e-06, t_loss=0.6544, t_f1r=[0.842, 0.950, 0.728, 0.797, 0.895, 0.792, 0.813], t_f1=0.831\n",
      "05/30 16:16:22 [INFO] eps=2, steps=6970/32524, lr=3.41e-06, t_loss=0.6516, t_f1r=[0.843, 0.951, 0.730, 0.799, 0.896, 0.793, 0.815], t_f1=0.832\n",
      "05/30 16:17:25 [INFO] eps=2, steps=7200/32524, lr=2.91e-06, t_loss=0.6500, t_f1r=[0.844, 0.951, 0.732, 0.799, 0.897, 0.794, 0.815], t_f1=0.833\n",
      "05/30 16:18:25 [INFO] eps=2, steps=7420/32524, lr=2.53e-06, t_loss=0.6480, t_f1r=[0.845, 0.952, 0.733, 0.800, 0.897, 0.795, 0.815], t_f1=0.834\n",
      "05/30 16:19:25 [INFO] eps=2, steps=7640/32524, lr=2.25e-06, t_loss=0.6455, t_f1r=[0.846, 0.952, 0.735, 0.801, 0.897, 0.797, 0.816], t_f1=0.835\n",
      "05/30 16:20:26 [INFO] eps=2, steps=7860/32524, lr=2.08e-06, t_loss=0.6431, t_f1r=[0.848, 0.953, 0.736, 0.803, 0.898, 0.798, 0.816], t_f1=0.836\n",
      "05/30 16:21:26 [INFO] eps=2, steps=8080/32524, lr=2e-06, t_loss=0.6413, t_f1r=[0.848, 0.953, 0.737, 0.803, 0.898, 0.800, 0.817], t_f1=0.837\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08547721719942498ef046fdce0a3f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 16:22:40 [INFO] steps=324/2376, v_loss=0.5166, v_f1r=[0.946, 0.000, 0.000, 0.000, 0.000, 0.000, 0.863], v_f1=0.258\n",
      "05/30 16:23:40 [INFO] steps=656/2376, v_loss=0.5090, v_f1r=[0.938, 0.976, 0.000, 0.000, 0.000, 0.000, 0.892], v_f1=0.401\n",
      "05/30 16:24:40 [INFO] steps=988/2376, v_loss=0.5688, v_f1r=[0.916, 0.970, 0.732, 0.000, 0.000, 0.865, 0.868], v_f1=0.622\n",
      "05/30 16:25:41 [INFO] steps=1319/2376, v_loss=0.5795, v_f1r=[0.889, 0.959, 0.693, 0.000, 0.000, 0.893, 0.851], v_f1=0.612\n",
      "05/30 16:26:41 [INFO] steps=1650/2376, v_loss=0.5495, v_f1r=[0.887, 0.959, 0.662, 0.000, 0.944, 0.894, 0.839], v_f1=0.741\n",
      "05/30 16:27:41 [INFO] steps=1981/2376, v_loss=0.5348, v_f1r=[0.885, 0.958, 0.624, 0.668, 0.952, 0.893, 0.827], v_f1=0.829\n",
      "05/30 16:28:41 [INFO] steps=2312/2376, v_loss=0.5447, v_f1r=[0.882, 0.956, 0.563, 0.864, 0.947, 0.888, 0.810], v_f1=0.844\n",
      "05/30 16:28:53 [INFO] eps=2, steps=8131/32524, lr=2.84e-05, t_loss=0.6409, v_f1r=[0.881, 0.956, 0.553, 0.871, 0.946, 0.887, 0.807], v_loss=0.5463*, v_f1=0.8427*\n",
      "05/30 16:29:15 [INFO] eps=2, steps=8140/32524, lr=2.84e-05, t_loss=0.5529, t_f1r=[0.833, 0.966, 0.766, 0.875, 0.936, 0.919, 0.955], t_f1=0.893\n",
      "05/30 16:30:16 [INFO] eps=2, steps=8370/32524, lr=2.84e-05, t_loss=0.6621, t_f1r=[0.822, 0.948, 0.725, 0.791, 0.912, 0.773, 0.795], t_f1=0.824\n",
      "05/30 16:31:18 [INFO] eps=2, steps=8600/32524, lr=2.82e-05, t_loss=0.6648, t_f1r=[0.833, 0.947, 0.727, 0.784, 0.908, 0.774, 0.804], t_f1=0.825\n",
      "05/30 16:32:20 [INFO] eps=2, steps=8830/32524, lr=2.8e-05, t_loss=0.6620, t_f1r=[0.835, 0.943, 0.722, 0.796, 0.909, 0.779, 0.808], t_f1=0.827\n",
      "05/30 16:33:21 [INFO] eps=2, steps=9060/32524, lr=2.76e-05, t_loss=0.6647, t_f1r=[0.840, 0.943, 0.715, 0.791, 0.902, 0.780, 0.811], t_f1=0.826\n",
      "05/30 16:34:23 [INFO] eps=2, steps=9290/32524, lr=2.71e-05, t_loss=0.6661, t_f1r=[0.838, 0.944, 0.713, 0.786, 0.905, 0.783, 0.810], t_f1=0.825\n",
      "05/30 16:35:25 [INFO] eps=2, steps=9520/32524, lr=2.66e-05, t_loss=0.6685, t_f1r=[0.837, 0.946, 0.713, 0.786, 0.902, 0.784, 0.808], t_f1=0.825\n",
      "05/30 16:36:27 [INFO] eps=2, steps=9750/32524, lr=2.59e-05, t_loss=0.6700, t_f1r=[0.838, 0.946, 0.715, 0.786, 0.900, 0.781, 0.806], t_f1=0.824\n",
      "05/30 16:37:29 [INFO] eps=2, steps=9980/32524, lr=2.52e-05, t_loss=0.6698, t_f1r=[0.841, 0.946, 0.716, 0.786, 0.898, 0.781, 0.805], t_f1=0.825\n",
      "05/30 16:38:31 [INFO] eps=2, steps=10210/32524, lr=2.44e-05, t_loss=0.6694, t_f1r=[0.840, 0.945, 0.717, 0.787, 0.898, 0.781, 0.807], t_f1=0.825\n",
      "05/30 16:39:33 [INFO] eps=2, steps=10440/32524, lr=2.35e-05, t_loss=0.6694, t_f1r=[0.840, 0.946, 0.716, 0.789, 0.898, 0.779, 0.806], t_f1=0.825\n",
      "05/30 16:40:35 [INFO] eps=2, steps=10670/32524, lr=2.26e-05, t_loss=0.6691, t_f1r=[0.838, 0.947, 0.718, 0.788, 0.898, 0.779, 0.807], t_f1=0.825\n",
      "05/30 16:41:37 [INFO] eps=2, steps=10900/32524, lr=2.16e-05, t_loss=0.6691, t_f1r=[0.836, 0.948, 0.719, 0.785, 0.898, 0.778, 0.808], t_f1=0.825\n",
      "05/30 16:42:39 [INFO] eps=2, steps=11130/32524, lr=2.05e-05, t_loss=0.6687, t_f1r=[0.838, 0.947, 0.720, 0.785, 0.897, 0.778, 0.807], t_f1=0.825\n",
      "05/30 16:43:41 [INFO] eps=2, steps=11360/32524, lr=1.94e-05, t_loss=0.6671, t_f1r=[0.840, 0.947, 0.720, 0.787, 0.897, 0.779, 0.807], t_f1=0.825\n",
      "05/30 16:44:43 [INFO] eps=2, steps=11590/32524, lr=1.83e-05, t_loss=0.6646, t_f1r=[0.840, 0.948, 0.721, 0.788, 0.897, 0.781, 0.808], t_f1=0.826\n",
      "05/30 16:45:45 [INFO] eps=2, steps=11820/32524, lr=1.71e-05, t_loss=0.6629, t_f1r=[0.841, 0.948, 0.722, 0.787, 0.898, 0.782, 0.810], t_f1=0.827\n",
      "05/30 16:46:48 [INFO] eps=2, steps=12050/32524, lr=1.6e-05, t_loss=0.6607, t_f1r=[0.842, 0.949, 0.724, 0.789, 0.899, 0.784, 0.809], t_f1=0.828\n",
      "05/30 16:47:50 [INFO] eps=2, steps=12280/32524, lr=1.48e-05, t_loss=0.6583, t_f1r=[0.844, 0.949, 0.726, 0.791, 0.900, 0.785, 0.809], t_f1=0.829\n",
      "05/30 16:48:52 [INFO] eps=2, steps=12510/32524, lr=1.36e-05, t_loss=0.6564, t_f1r=[0.845, 0.949, 0.727, 0.792, 0.900, 0.785, 0.809], t_f1=0.830\n",
      "05/30 16:49:54 [INFO] eps=2, steps=12740/32524, lr=1.25e-05, t_loss=0.6538, t_f1r=[0.846, 0.948, 0.730, 0.793, 0.900, 0.786, 0.810], t_f1=0.831\n",
      "05/30 16:50:57 [INFO] eps=2, steps=12970/32524, lr=1.13e-05, t_loss=0.6506, t_f1r=[0.848, 0.949, 0.732, 0.795, 0.900, 0.789, 0.811], t_f1=0.832\n",
      "05/30 16:51:59 [INFO] eps=2, steps=13200/32524, lr=1.02e-05, t_loss=0.6479, t_f1r=[0.850, 0.950, 0.734, 0.796, 0.901, 0.789, 0.812], t_f1=0.833\n",
      "05/30 16:53:02 [INFO] eps=2, steps=13430/32524, lr=9.15e-06, t_loss=0.6454, t_f1r=[0.850, 0.951, 0.736, 0.797, 0.901, 0.790, 0.813], t_f1=0.834\n",
      "05/30 16:54:04 [INFO] eps=2, steps=13660/32524, lr=8.14e-06, t_loss=0.6430, t_f1r=[0.852, 0.951, 0.739, 0.797, 0.901, 0.792, 0.814], t_f1=0.835\n",
      "05/30 16:55:07 [INFO] eps=2, steps=13890/32524, lr=7.17e-06, t_loss=0.6397, t_f1r=[0.853, 0.952, 0.740, 0.799, 0.901, 0.793, 0.815], t_f1=0.836\n",
      "05/30 16:56:09 [INFO] eps=2, steps=14120/32524, lr=6.27e-06, t_loss=0.6372, t_f1r=[0.855, 0.953, 0.742, 0.800, 0.901, 0.795, 0.816], t_f1=0.838\n",
      "05/30 16:57:12 [INFO] eps=2, steps=14350/32524, lr=5.45e-06, t_loss=0.6339, t_f1r=[0.856, 0.954, 0.745, 0.802, 0.902, 0.796, 0.818], t_f1=0.839\n",
      "05/30 16:58:14 [INFO] eps=2, steps=14580/32524, lr=4.69e-06, t_loss=0.6313, t_f1r=[0.858, 0.954, 0.747, 0.803, 0.903, 0.798, 0.819], t_f1=0.840\n",
      "05/30 16:59:17 [INFO] eps=2, steps=14810/32524, lr=4.03e-06, t_loss=0.6285, t_f1r=[0.859, 0.955, 0.749, 0.804, 0.903, 0.799, 0.820], t_f1=0.841\n",
      "05/30 17:00:20 [INFO] eps=2, steps=15040/32524, lr=3.45e-06, t_loss=0.6263, t_f1r=[0.860, 0.955, 0.749, 0.805, 0.904, 0.801, 0.821], t_f1=0.842\n",
      "05/30 17:01:22 [INFO] eps=2, steps=15270/32524, lr=2.96e-06, t_loss=0.6232, t_f1r=[0.861, 0.956, 0.752, 0.806, 0.905, 0.802, 0.823], t_f1=0.844\n",
      "05/30 17:02:22 [INFO] eps=2, steps=15490/32524, lr=2.58e-06, t_loss=0.6207, t_f1r=[0.862, 0.956, 0.754, 0.808, 0.905, 0.804, 0.824], t_f1=0.845\n",
      "05/30 17:03:22 [INFO] eps=2, steps=15710/32524, lr=2.3e-06, t_loss=0.6187, t_f1r=[0.863, 0.957, 0.755, 0.809, 0.906, 0.805, 0.824], t_f1=0.846\n",
      "05/30 17:04:23 [INFO] eps=2, steps=15930/32524, lr=2.11e-06, t_loss=0.6161, t_f1r=[0.865, 0.957, 0.756, 0.810, 0.907, 0.806, 0.825], t_f1=0.847\n",
      "05/30 17:05:23 [INFO] eps=2, steps=16150/32524, lr=2.01e-06, t_loss=0.6144, t_f1r=[0.865, 0.958, 0.757, 0.811, 0.907, 0.808, 0.826], t_f1=0.847\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77026f397bb4f8fb2a25c2b85f8cbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 17:06:53 [INFO] steps=323/2376, v_loss=0.5040, v_f1r=[0.949, 0.000, 0.000, 0.000, 0.000, 0.000, 0.868], v_f1=0.260\n",
      "05/30 17:07:53 [INFO] steps=655/2376, v_loss=0.4976, v_f1r=[0.941, 0.976, 0.000, 0.000, 0.000, 0.000, 0.897], v_f1=0.402\n",
      "05/30 17:08:54 [INFO] steps=987/2376, v_loss=0.5528, v_f1r=[0.921, 0.971, 0.742, 0.000, 0.000, 0.872, 0.874], v_f1=0.626\n",
      "05/30 17:09:54 [INFO] steps=1318/2376, v_loss=0.5617, v_f1r=[0.895, 0.962, 0.704, 0.000, 0.000, 0.899, 0.856], v_f1=0.617\n",
      "05/30 17:10:54 [INFO] steps=1650/2376, v_loss=0.5386, v_f1r=[0.893, 0.961, 0.668, 0.000, 0.943, 0.900, 0.842], v_f1=0.744\n",
      "05/30 17:11:54 [INFO] steps=1980/2376, v_loss=0.5280, v_f1r=[0.892, 0.961, 0.627, 0.670, 0.949, 0.898, 0.828], v_f1=0.832\n",
      "05/30 17:12:54 [INFO] steps=2311/2376, v_loss=0.5364, v_f1r=[0.888, 0.960, 0.571, 0.868, 0.945, 0.892, 0.811], v_f1=0.848\n",
      "05/30 17:13:06 [INFO] eps=2, steps=16262/32524, lr=2.66e-05, t_loss=0.6135, v_f1r=[0.888, 0.959, 0.562, 0.875, 0.944, 0.892, 0.808], v_loss=0.5377*, v_f1=0.8469*\n",
      "05/30 17:13:08 [INFO] eps=2, steps=16270/32524, lr=2.66e-05, t_loss=0.5855, t_f1r=[0.923, 0.957, 0.690, 0.828, 0.692, 0.897, 0.690], t_f1=0.811\n",
      "05/30 17:14:08 [INFO] eps=2, steps=16420/32524, lr=2.66e-05, t_loss=0.6135, t_f1r=[0.854, 0.969, 0.776, 0.816, 0.901, 0.811, 0.838], t_f1=0.852\n",
      "05/30 17:15:10 [INFO] eps=2, steps=16650/32524, lr=2.64e-05, t_loss=0.6418, t_f1r=[0.848, 0.959, 0.741, 0.798, 0.904, 0.806, 0.822], t_f1=0.840\n",
      "05/30 17:16:11 [INFO] eps=2, steps=16880/32524, lr=2.62e-05, t_loss=0.6425, t_f1r=[0.851, 0.952, 0.730, 0.807, 0.898, 0.799, 0.820], t_f1=0.837\n",
      "05/30 17:17:13 [INFO] eps=2, steps=17110/32524, lr=2.59e-05, t_loss=0.6391, t_f1r=[0.849, 0.951, 0.734, 0.808, 0.901, 0.796, 0.820], t_f1=0.837\n",
      "05/30 17:18:15 [INFO] eps=2, steps=17340/32524, lr=2.55e-05, t_loss=0.6425, t_f1r=[0.845, 0.951, 0.732, 0.809, 0.897, 0.793, 0.819], t_f1=0.835\n",
      "05/30 17:19:16 [INFO] eps=2, steps=17570/32524, lr=2.5e-05, t_loss=0.6396, t_f1r=[0.847, 0.951, 0.735, 0.810, 0.896, 0.795, 0.822], t_f1=0.837\n",
      "05/30 17:20:18 [INFO] eps=2, steps=17800/32524, lr=2.45e-05, t_loss=0.6401, t_f1r=[0.850, 0.950, 0.737, 0.808, 0.894, 0.796, 0.820], t_f1=0.836\n",
      "05/30 17:21:20 [INFO] eps=2, steps=18030/32524, lr=2.38e-05, t_loss=0.6429, t_f1r=[0.851, 0.951, 0.736, 0.809, 0.894, 0.793, 0.816], t_f1=0.836\n",
      "05/30 17:22:22 [INFO] eps=2, steps=18260/32524, lr=2.31e-05, t_loss=0.6425, t_f1r=[0.851, 0.952, 0.737, 0.808, 0.896, 0.795, 0.818], t_f1=0.837\n",
      "05/30 17:23:24 [INFO] eps=2, steps=18490/32524, lr=2.23e-05, t_loss=0.6402, t_f1r=[0.851, 0.952, 0.740, 0.809, 0.898, 0.795, 0.818], t_f1=0.838\n",
      "05/30 17:24:26 [INFO] eps=2, steps=18720/32524, lr=2.14e-05, t_loss=0.6396, t_f1r=[0.849, 0.951, 0.741, 0.809, 0.899, 0.796, 0.819], t_f1=0.838\n",
      "05/30 17:25:28 [INFO] eps=2, steps=18950/32524, lr=2.05e-05, t_loss=0.6382, t_f1r=[0.849, 0.952, 0.743, 0.808, 0.900, 0.796, 0.820], t_f1=0.838\n",
      "05/30 17:26:30 [INFO] eps=2, steps=19180/32524, lr=1.96e-05, t_loss=0.6351, t_f1r=[0.851, 0.953, 0.743, 0.807, 0.901, 0.798, 0.821], t_f1=0.839\n",
      "05/30 17:27:32 [INFO] eps=2, steps=19410/32524, lr=1.86e-05, t_loss=0.6337, t_f1r=[0.852, 0.954, 0.744, 0.807, 0.901, 0.799, 0.821], t_f1=0.840\n",
      "05/30 17:28:34 [INFO] eps=2, steps=19640/32524, lr=1.75e-05, t_loss=0.6308, t_f1r=[0.853, 0.954, 0.745, 0.807, 0.902, 0.800, 0.822], t_f1=0.840\n",
      "05/30 17:29:36 [INFO] eps=2, steps=19870/32524, lr=1.65e-05, t_loss=0.6282, t_f1r=[0.854, 0.954, 0.748, 0.809, 0.903, 0.802, 0.822], t_f1=0.842\n",
      "05/30 17:30:39 [INFO] eps=2, steps=20100/32524, lr=1.54e-05, t_loss=0.6268, t_f1r=[0.855, 0.954, 0.749, 0.809, 0.903, 0.802, 0.822], t_f1=0.842\n",
      "05/30 17:31:41 [INFO] eps=2, steps=20330/32524, lr=1.43e-05, t_loss=0.6251, t_f1r=[0.856, 0.954, 0.750, 0.810, 0.904, 0.804, 0.823], t_f1=0.843\n",
      "05/30 17:32:43 [INFO] eps=2, steps=20560/32524, lr=1.32e-05, t_loss=0.6225, t_f1r=[0.858, 0.955, 0.752, 0.810, 0.905, 0.806, 0.824], t_f1=0.844\n",
      "05/30 17:33:45 [INFO] eps=2, steps=20790/32524, lr=1.21e-05, t_loss=0.6199, t_f1r=[0.859, 0.955, 0.755, 0.812, 0.905, 0.809, 0.824], t_f1=0.846\n",
      "05/30 17:34:48 [INFO] eps=2, steps=21020/32524, lr=1.1e-05, t_loss=0.6175, t_f1r=[0.861, 0.955, 0.757, 0.812, 0.905, 0.810, 0.825], t_f1=0.847\n",
      "05/30 17:35:50 [INFO] eps=2, steps=21250/32524, lr=1e-05, t_loss=0.6147, t_f1r=[0.862, 0.956, 0.761, 0.814, 0.905, 0.811, 0.827], t_f1=0.848\n",
      "05/30 17:36:53 [INFO] eps=2, steps=21480/32524, lr=9e-06, t_loss=0.6128, t_f1r=[0.863, 0.957, 0.762, 0.815, 0.906, 0.812, 0.828], t_f1=0.849\n",
      "05/30 17:37:55 [INFO] eps=2, steps=21710/32524, lr=8.03e-06, t_loss=0.6104, t_f1r=[0.864, 0.957, 0.763, 0.815, 0.906, 0.815, 0.828], t_f1=0.850\n",
      "05/30 17:38:58 [INFO] eps=2, steps=21940/32524, lr=7.12e-06, t_loss=0.6082, t_f1r=[0.865, 0.958, 0.766, 0.817, 0.907, 0.815, 0.829], t_f1=0.851\n",
      "05/30 17:40:00 [INFO] eps=2, steps=22170/32524, lr=6.26e-06, t_loss=0.6053, t_f1r=[0.867, 0.958, 0.767, 0.819, 0.907, 0.817, 0.830], t_f1=0.852\n",
      "05/30 17:41:03 [INFO] eps=2, steps=22400/32524, lr=5.47e-06, t_loss=0.6026, t_f1r=[0.868, 0.959, 0.769, 0.819, 0.908, 0.818, 0.831], t_f1=0.853\n",
      "05/30 17:42:05 [INFO] eps=2, steps=22630/32524, lr=4.74e-06, t_loss=0.6007, t_f1r=[0.869, 0.959, 0.771, 0.820, 0.908, 0.819, 0.833], t_f1=0.854\n",
      "05/30 17:43:08 [INFO] eps=2, steps=22860/32524, lr=4.09e-06, t_loss=0.5982, t_f1r=[0.870, 0.959, 0.772, 0.821, 0.909, 0.820, 0.834], t_f1=0.855\n",
      "05/30 17:44:11 [INFO] eps=2, steps=23090/32524, lr=3.53e-06, t_loss=0.5964, t_f1r=[0.871, 0.960, 0.773, 0.821, 0.909, 0.822, 0.835], t_f1=0.856\n",
      "05/30 17:45:13 [INFO] eps=2, steps=23320/32524, lr=3.04e-06, t_loss=0.5947, t_f1r=[0.872, 0.960, 0.775, 0.823, 0.909, 0.823, 0.836], t_f1=0.857\n",
      "05/30 17:46:13 [INFO] eps=2, steps=23540/32524, lr=2.66e-06, t_loss=0.5930, t_f1r=[0.873, 0.961, 0.776, 0.823, 0.909, 0.823, 0.837], t_f1=0.858\n",
      "05/30 17:47:14 [INFO] eps=2, steps=23760/32524, lr=2.37e-06, t_loss=0.5915, t_f1r=[0.874, 0.961, 0.777, 0.824, 0.910, 0.824, 0.837], t_f1=0.858\n",
      "05/30 17:48:14 [INFO] eps=2, steps=23980/32524, lr=2.16e-06, t_loss=0.5893, t_f1r=[0.874, 0.961, 0.779, 0.825, 0.910, 0.826, 0.838], t_f1=0.859\n",
      "05/30 17:49:14 [INFO] eps=2, steps=24200/32524, lr=2.03e-06, t_loss=0.5871, t_f1r=[0.875, 0.962, 0.780, 0.826, 0.911, 0.827, 0.839], t_f1=0.860\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f24277d2254b279b6bd7fceed95f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 17:51:07 [INFO] steps=323/2376, v_loss=0.5095, v_f1r=[0.948, 0.000, 0.000, 0.000, 0.000, 0.000, 0.865], v_f1=0.259\n",
      "05/30 17:52:07 [INFO] steps=655/2376, v_loss=0.5059, v_f1r=[0.940, 0.977, 0.000, 0.000, 0.000, 0.000, 0.895], v_f1=0.402\n",
      "05/30 17:53:07 [INFO] steps=987/2376, v_loss=0.5535, v_f1r=[0.923, 0.973, 0.743, 0.000, 0.000, 0.876, 0.873], v_f1=0.627\n",
      "05/30 17:54:07 [INFO] steps=1319/2376, v_loss=0.5566, v_f1r=[0.901, 0.967, 0.701, 0.000, 0.000, 0.905, 0.858], v_f1=0.619\n",
      "05/30 17:55:07 [INFO] steps=1650/2376, v_loss=0.5299, v_f1r=[0.900, 0.966, 0.670, 0.000, 0.946, 0.905, 0.846], v_f1=0.748\n",
      "05/30 17:56:08 [INFO] steps=1982/2376, v_loss=0.5160, v_f1r=[0.899, 0.966, 0.633, 0.677, 0.953, 0.904, 0.835], v_f1=0.838\n",
      "05/30 17:57:08 [INFO] steps=2313/2376, v_loss=0.5213, v_f1r=[0.896, 0.965, 0.584, 0.876, 0.948, 0.899, 0.821], v_f1=0.856\n",
      "05/30 17:57:19 [INFO] eps=2, steps=24393/32524, lr=2.49e-05, t_loss=0.5857, v_f1r=[0.896, 0.965, 0.576, 0.884, 0.948, 0.898, 0.818], v_loss=0.5219*, v_f1=0.8547*\n",
      "05/30 17:57:41 [INFO] eps=2, steps=24400/32524, lr=2.49e-05, t_loss=0.5155, t_f1r=[0.917, 1.000, 0.791, 0.815, 0.923, 0.889, 0.800], t_f1=0.876\n",
      "05/30 17:58:42 [INFO] eps=2, steps=24630/32524, lr=2.48e-05, t_loss=0.5736, t_f1r=[0.878, 0.968, 0.774, 0.843, 0.928, 0.824, 0.858], t_f1=0.868\n",
      "05/30 17:59:44 [INFO] eps=2, steps=24860/32524, lr=2.47e-05, t_loss=0.5940, t_f1r=[0.868, 0.963, 0.773, 0.828, 0.916, 0.820, 0.836], t_f1=0.858\n",
      "05/30 18:00:46 [INFO] eps=2, steps=25090/32524, lr=2.45e-05, t_loss=0.6047, t_f1r=[0.863, 0.959, 0.768, 0.823, 0.914, 0.811, 0.825], t_f1=0.852\n",
      "05/30 18:01:47 [INFO] eps=2, steps=25320/32524, lr=2.41e-05, t_loss=0.6089, t_f1r=[0.858, 0.958, 0.766, 0.820, 0.911, 0.803, 0.831], t_f1=0.850\n",
      "05/30 18:02:49 [INFO] eps=2, steps=25550/32524, lr=2.37e-05, t_loss=0.6088, t_f1r=[0.857, 0.959, 0.764, 0.817, 0.912, 0.805, 0.829], t_f1=0.849\n",
      "05/30 18:03:51 [INFO] eps=2, steps=25780/32524, lr=2.33e-05, t_loss=0.6096, t_f1r=[0.859, 0.957, 0.763, 0.818, 0.911, 0.810, 0.829], t_f1=0.850\n",
      "05/30 18:04:53 [INFO] eps=2, steps=26010/32524, lr=2.27e-05, t_loss=0.6099, t_f1r=[0.860, 0.956, 0.764, 0.818, 0.911, 0.813, 0.827], t_f1=0.850\n",
      "05/30 18:05:55 [INFO] eps=2, steps=26240/32524, lr=2.21e-05, t_loss=0.6086, t_f1r=[0.860, 0.957, 0.765, 0.819, 0.911, 0.810, 0.830], t_f1=0.850\n",
      "05/30 18:06:56 [INFO] eps=2, steps=26470/32524, lr=2.14e-05, t_loss=0.6065, t_f1r=[0.859, 0.956, 0.767, 0.822, 0.911, 0.811, 0.830], t_f1=0.851\n",
      "05/30 18:07:58 [INFO] eps=2, steps=26700/32524, lr=2.06e-05, t_loss=0.6061, t_f1r=[0.859, 0.956, 0.767, 0.822, 0.912, 0.814, 0.832], t_f1=0.852\n",
      "05/30 18:09:01 [INFO] eps=2, steps=26930/32524, lr=1.98e-05, t_loss=0.6050, t_f1r=[0.861, 0.957, 0.766, 0.824, 0.911, 0.815, 0.832], t_f1=0.852\n",
      "05/30 18:10:03 [INFO] eps=2, steps=27160/32524, lr=1.89e-05, t_loss=0.6047, t_f1r=[0.862, 0.957, 0.767, 0.823, 0.910, 0.814, 0.831], t_f1=0.852\n",
      "05/30 18:11:05 [INFO] eps=2, steps=27390/32524, lr=1.8e-05, t_loss=0.6038, t_f1r=[0.864, 0.958, 0.767, 0.821, 0.909, 0.816, 0.831], t_f1=0.852\n",
      "05/30 18:12:07 [INFO] eps=2, steps=27620/32524, lr=1.71e-05, t_loss=0.6019, t_f1r=[0.865, 0.958, 0.768, 0.823, 0.910, 0.816, 0.834], t_f1=0.853\n",
      "05/30 18:13:09 [INFO] eps=2, steps=27850/32524, lr=1.61e-05, t_loss=0.5998, t_f1r=[0.865, 0.958, 0.767, 0.823, 0.910, 0.818, 0.835], t_f1=0.854\n",
      "05/30 18:14:11 [INFO] eps=2, steps=28080/32524, lr=1.51e-05, t_loss=0.5982, t_f1r=[0.866, 0.959, 0.769, 0.824, 0.910, 0.818, 0.835], t_f1=0.854\n",
      "05/30 18:15:14 [INFO] eps=2, steps=28310/32524, lr=1.41e-05, t_loss=0.5973, t_f1r=[0.866, 0.959, 0.770, 0.824, 0.910, 0.819, 0.835], t_f1=0.855\n",
      "05/30 18:16:16 [INFO] eps=2, steps=28540/32524, lr=1.31e-05, t_loss=0.5957, t_f1r=[0.866, 0.959, 0.773, 0.824, 0.910, 0.820, 0.836], t_f1=0.855\n",
      "05/30 18:17:18 [INFO] eps=2, steps=28770/32524, lr=1.21e-05, t_loss=0.5939, t_f1r=[0.867, 0.960, 0.774, 0.823, 0.911, 0.821, 0.836], t_f1=0.856\n",
      "05/30 18:18:20 [INFO] eps=2, steps=29000/32524, lr=1.11e-05, t_loss=0.5916, t_f1r=[0.869, 0.961, 0.776, 0.825, 0.911, 0.823, 0.836], t_f1=0.857\n",
      "05/30 18:19:23 [INFO] eps=2, steps=29230/32524, lr=1.01e-05, t_loss=0.5907, t_f1r=[0.869, 0.961, 0.778, 0.825, 0.912, 0.823, 0.837], t_f1=0.858\n",
      "05/30 18:20:25 [INFO] eps=2, steps=29460/32524, lr=9.12e-06, t_loss=0.5893, t_f1r=[0.870, 0.961, 0.780, 0.825, 0.912, 0.823, 0.838], t_f1=0.858\n",
      "05/30 18:21:28 [INFO] eps=2, steps=29690/32524, lr=8.2e-06, t_loss=0.5865, t_f1r=[0.872, 0.961, 0.782, 0.826, 0.913, 0.825, 0.838], t_f1=0.859\n",
      "05/30 18:22:30 [INFO] eps=2, steps=29920/32524, lr=7.31e-06, t_loss=0.5844, t_f1r=[0.873, 0.962, 0.784, 0.827, 0.913, 0.826, 0.839], t_f1=0.861\n",
      "05/30 18:23:33 [INFO] eps=2, steps=30150/32524, lr=6.48e-06, t_loss=0.5826, t_f1r=[0.874, 0.962, 0.786, 0.828, 0.913, 0.827, 0.840], t_f1=0.861\n",
      "05/30 18:24:35 [INFO] eps=2, steps=30380/32524, lr=5.7e-06, t_loss=0.5813, t_f1r=[0.875, 0.963, 0.787, 0.828, 0.914, 0.828, 0.841], t_f1=0.862\n",
      "05/30 18:25:38 [INFO] eps=2, steps=30610/32524, lr=4.99e-06, t_loss=0.5789, t_f1r=[0.876, 0.963, 0.789, 0.829, 0.914, 0.828, 0.841], t_f1=0.863\n",
      "05/30 18:26:40 [INFO] eps=2, steps=30840/32524, lr=4.34e-06, t_loss=0.5776, t_f1r=[0.876, 0.963, 0.790, 0.830, 0.914, 0.830, 0.841], t_f1=0.864\n",
      "05/30 18:27:43 [INFO] eps=2, steps=31070/32524, lr=3.76e-06, t_loss=0.5754, t_f1r=[0.878, 0.963, 0.791, 0.830, 0.914, 0.832, 0.842], t_f1=0.864\n",
      "05/30 18:28:45 [INFO] eps=2, steps=31300/32524, lr=3.25e-06, t_loss=0.5732, t_f1r=[0.879, 0.964, 0.792, 0.831, 0.915, 0.833, 0.843], t_f1=0.865\n",
      "05/30 18:29:48 [INFO] eps=2, steps=31530/32524, lr=2.83e-06, t_loss=0.5719, t_f1r=[0.879, 0.964, 0.793, 0.832, 0.915, 0.834, 0.843], t_f1=0.866\n",
      "05/30 18:30:48 [INFO] eps=2, steps=31750/32524, lr=2.51e-06, t_loss=0.5702, t_f1r=[0.881, 0.964, 0.794, 0.833, 0.915, 0.835, 0.844], t_f1=0.867\n",
      "05/30 18:31:48 [INFO] eps=2, steps=31970/32524, lr=2.26e-06, t_loss=0.5686, t_f1r=[0.881, 0.965, 0.796, 0.834, 0.915, 0.835, 0.845], t_f1=0.867\n",
      "05/30 18:32:48 [INFO] eps=2, steps=32190/32524, lr=2.1e-06, t_loss=0.5676, t_f1r=[0.882, 0.965, 0.797, 0.834, 0.915, 0.835, 0.845], t_f1=0.868\n",
      "05/30 18:33:49 [INFO] eps=2, steps=32410/32524, lr=2.01e-06, t_loss=0.5661, t_f1r=[0.883, 0.965, 0.798, 0.834, 0.916, 0.836, 0.846], t_f1=0.868\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55388d86cdb94da794fe2cf72dc3e8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 18:35:20 [INFO] steps=325/2376, v_loss=0.4806, v_f1r=[0.952, 0.000, 0.000, 0.000, 0.000, 0.000, 0.874], v_f1=0.261\n",
      "05/30 18:36:20 [INFO] steps=657/2376, v_loss=0.4813, v_f1r=[0.945, 0.977, 0.000, 0.000, 0.000, 0.000, 0.904], v_f1=0.404\n",
      "05/30 18:37:20 [INFO] steps=988/2376, v_loss=0.5378, v_f1r=[0.927, 0.974, 0.759, 0.000, 0.000, 0.883, 0.880], v_f1=0.632\n",
      "05/30 18:38:20 [INFO] steps=1319/2376, v_loss=0.5402, v_f1r=[0.908, 0.968, 0.726, 0.000, 0.000, 0.913, 0.863], v_f1=0.625\n",
      "05/30 18:39:20 [INFO] steps=1650/2376, v_loss=0.5147, v_f1r=[0.906, 0.968, 0.698, 0.000, 0.948, 0.913, 0.851], v_f1=0.755\n",
      "05/30 18:40:21 [INFO] steps=1981/2376, v_loss=0.5022, v_f1r=[0.905, 0.967, 0.664, 0.690, 0.955, 0.911, 0.839], v_f1=0.847\n",
      "05/30 18:41:21 [INFO] steps=2312/2376, v_loss=0.5081, v_f1r=[0.902, 0.967, 0.615, 0.881, 0.951, 0.906, 0.823], v_f1=0.864\n",
      "05/30 18:41:32 [INFO] eps=2, steps=32524/32524, lr=2.33e-05, t_loss=0.5655, v_f1r=[0.902, 0.967, 0.607, 0.889, 0.950, 0.904, 0.821], v_loss=0.5088*, v_f1=0.8627*\n",
      "05/30 18:41:32 [INFO] already max rock_type : 4\n",
      "05/30 18:41:33 [INFO] load_img_size=448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a1f948b8c24ce19235d3c34a7354b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/32524 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 18:42:35 [INFO] eps=3, steps=150/32524, lr=2.32e-05, t_loss=0.5527, t_f1r=[0.887, 0.954, 0.828, 0.860, 0.927, 0.830, 0.841], t_f1=0.875\n",
      "05/30 18:43:36 [INFO] eps=3, steps=380/32524, lr=2.31e-05, t_loss=0.5705, t_f1r=[0.887, 0.956, 0.792, 0.828, 0.918, 0.823, 0.840], t_f1=0.863\n",
      "05/30 18:44:37 [INFO] eps=3, steps=610/32524, lr=2.3e-05, t_loss=0.5699, t_f1r=[0.882, 0.956, 0.802, 0.835, 0.917, 0.835, 0.839], t_f1=0.867\n",
      "05/30 18:45:39 [INFO] eps=3, steps=840/32524, lr=2.27e-05, t_loss=0.5702, t_f1r=[0.884, 0.960, 0.800, 0.834, 0.916, 0.839, 0.836], t_f1=0.867\n",
      "05/30 18:46:40 [INFO] eps=3, steps=1070/32524, lr=2.24e-05, t_loss=0.5749, t_f1r=[0.879, 0.959, 0.788, 0.833, 0.916, 0.834, 0.840], t_f1=0.864\n",
      "05/30 18:47:41 [INFO] eps=3, steps=1300/32524, lr=2.2e-05, t_loss=0.5759, t_f1r=[0.880, 0.960, 0.788, 0.830, 0.917, 0.832, 0.839], t_f1=0.864\n",
      "05/30 18:48:43 [INFO] eps=3, steps=1530/32524, lr=2.15e-05, t_loss=0.5760, t_f1r=[0.878, 0.961, 0.788, 0.829, 0.918, 0.831, 0.844], t_f1=0.864\n",
      "05/30 18:49:45 [INFO] eps=3, steps=1760/32524, lr=2.09e-05, t_loss=0.5782, t_f1r=[0.876, 0.960, 0.783, 0.828, 0.917, 0.832, 0.842], t_f1=0.863\n",
      "05/30 18:50:46 [INFO] eps=3, steps=1990/32524, lr=2.03e-05, t_loss=0.5777, t_f1r=[0.878, 0.961, 0.784, 0.830, 0.913, 0.832, 0.842], t_f1=0.863\n",
      "05/30 18:51:48 [INFO] eps=3, steps=2220/32524, lr=1.96e-05, t_loss=0.5784, t_f1r=[0.877, 0.960, 0.783, 0.830, 0.913, 0.830, 0.842], t_f1=0.862\n",
      "05/30 18:52:49 [INFO] eps=3, steps=2450/32524, lr=1.88e-05, t_loss=0.5785, t_f1r=[0.878, 0.960, 0.783, 0.830, 0.913, 0.830, 0.841], t_f1=0.862\n",
      "05/30 18:53:51 [INFO] eps=3, steps=2680/32524, lr=1.81e-05, t_loss=0.5792, t_f1r=[0.878, 0.959, 0.783, 0.830, 0.912, 0.829, 0.840], t_f1=0.862\n",
      "05/30 18:54:53 [INFO] eps=3, steps=2910/32524, lr=1.72e-05, t_loss=0.5772, t_f1r=[0.879, 0.960, 0.786, 0.831, 0.912, 0.830, 0.841], t_f1=0.863\n",
      "05/30 18:55:55 [INFO] eps=3, steps=3140/32524, lr=1.64e-05, t_loss=0.5757, t_f1r=[0.880, 0.959, 0.788, 0.831, 0.912, 0.832, 0.842], t_f1=0.863\n",
      "05/30 18:56:57 [INFO] eps=3, steps=3370/32524, lr=1.55e-05, t_loss=0.5752, t_f1r=[0.882, 0.960, 0.790, 0.832, 0.910, 0.832, 0.840], t_f1=0.864\n",
      "05/30 18:57:59 [INFO] eps=3, steps=3600/32524, lr=1.45e-05, t_loss=0.5735, t_f1r=[0.883, 0.960, 0.791, 0.832, 0.911, 0.833, 0.841], t_f1=0.864\n",
      "05/30 18:59:01 [INFO] eps=3, steps=3830/32524, lr=1.36e-05, t_loss=0.5718, t_f1r=[0.884, 0.961, 0.792, 0.833, 0.912, 0.834, 0.842], t_f1=0.865\n",
      "05/30 19:00:02 [INFO] eps=3, steps=3980/32524, lr=1.3e-05, t_loss=0.5698, t_f1r=[0.884, 0.961, 0.794, 0.834, 0.913, 0.835, 0.844], t_f1=0.867\n",
      "05/30 19:01:04 [INFO] eps=3, steps=4210/32524, lr=1.2e-05, t_loss=0.5674, t_f1r=[0.885, 0.962, 0.796, 0.836, 0.913, 0.837, 0.845], t_f1=0.868\n",
      "05/30 19:02:06 [INFO] eps=3, steps=4440/32524, lr=1.11e-05, t_loss=0.5656, t_f1r=[0.886, 0.963, 0.798, 0.837, 0.913, 0.838, 0.845], t_f1=0.869\n",
      "05/30 19:03:08 [INFO] eps=3, steps=4670/32524, lr=1.02e-05, t_loss=0.5635, t_f1r=[0.887, 0.963, 0.800, 0.838, 0.914, 0.838, 0.847], t_f1=0.870\n",
      "05/30 19:04:11 [INFO] eps=3, steps=4900/32524, lr=9.26e-06, t_loss=0.5623, t_f1r=[0.888, 0.963, 0.801, 0.839, 0.915, 0.839, 0.848], t_f1=0.870\n",
      "05/30 19:05:13 [INFO] eps=3, steps=5130/32524, lr=8.38e-06, t_loss=0.5602, t_f1r=[0.889, 0.964, 0.802, 0.840, 0.915, 0.840, 0.849], t_f1=0.871\n",
      "05/30 19:06:15 [INFO] eps=3, steps=5360/32524, lr=7.53e-06, t_loss=0.5586, t_f1r=[0.889, 0.965, 0.803, 0.840, 0.915, 0.841, 0.850], t_f1=0.872\n",
      "05/30 19:07:18 [INFO] eps=3, steps=5590/32524, lr=6.73e-06, t_loss=0.5565, t_f1r=[0.890, 0.965, 0.805, 0.841, 0.915, 0.842, 0.851], t_f1=0.873\n",
      "05/30 19:08:20 [INFO] eps=3, steps=5820/32524, lr=5.96e-06, t_loss=0.5550, t_f1r=[0.891, 0.966, 0.807, 0.841, 0.915, 0.843, 0.852], t_f1=0.874\n",
      "05/30 19:09:23 [INFO] eps=3, steps=6050/32524, lr=5.26e-06, t_loss=0.5523, t_f1r=[0.892, 0.966, 0.809, 0.843, 0.916, 0.845, 0.853], t_f1=0.875\n",
      "05/30 19:10:25 [INFO] eps=3, steps=6280/32524, lr=4.61e-06, t_loss=0.5501, t_f1r=[0.893, 0.967, 0.810, 0.844, 0.917, 0.847, 0.854], t_f1=0.876\n",
      "05/30 19:11:28 [INFO] eps=3, steps=6510/32524, lr=4.02e-06, t_loss=0.5479, t_f1r=[0.894, 0.968, 0.811, 0.845, 0.917, 0.848, 0.855], t_f1=0.877\n",
      "05/30 19:12:30 [INFO] eps=3, steps=6740/32524, lr=3.5e-06, t_loss=0.5466, t_f1r=[0.895, 0.968, 0.813, 0.845, 0.917, 0.849, 0.856], t_f1=0.878\n",
      "05/30 19:13:33 [INFO] eps=3, steps=6970/32524, lr=3.05e-06, t_loss=0.5447, t_f1r=[0.896, 0.968, 0.815, 0.845, 0.918, 0.850, 0.857], t_f1=0.878\n",
      "05/30 19:14:33 [INFO] eps=3, steps=7190/32524, lr=2.7e-06, t_loss=0.5434, t_f1r=[0.897, 0.968, 0.816, 0.846, 0.919, 0.850, 0.857], t_f1=0.879\n",
      "05/30 19:15:33 [INFO] eps=3, steps=7410/32524, lr=2.41e-06, t_loss=0.5422, t_f1r=[0.897, 0.968, 0.817, 0.847, 0.919, 0.851, 0.858], t_f1=0.880\n",
      "05/30 19:16:33 [INFO] eps=3, steps=7630/32524, lr=2.2e-06, t_loss=0.5404, t_f1r=[0.898, 0.969, 0.818, 0.848, 0.920, 0.851, 0.859], t_f1=0.880\n",
      "05/30 19:17:33 [INFO] eps=3, steps=7850/32524, lr=2.06e-06, t_loss=0.5391, t_f1r=[0.898, 0.969, 0.820, 0.848, 0.920, 0.851, 0.859], t_f1=0.881\n",
      "05/30 19:18:33 [INFO] eps=3, steps=8070/32524, lr=2e-06, t_loss=0.5378, t_f1r=[0.899, 0.969, 0.821, 0.849, 0.920, 0.852, 0.860], t_f1=0.881\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb8b51cc2b34a02b15e7688cccf990b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 19:19:50 [INFO] steps=323/2376, v_loss=0.4788, v_f1r=[0.957, 0.000, 0.000, 0.000, 0.000, 0.000, 0.877], v_f1=0.262\n",
      "05/30 19:20:50 [INFO] steps=655/2376, v_loss=0.4880, v_f1r=[0.949, 0.977, 0.000, 0.000, 0.000, 0.000, 0.899], v_f1=0.404\n",
      "05/30 19:21:50 [INFO] steps=986/2376, v_loss=0.5404, v_f1r=[0.932, 0.974, 0.760, 0.000, 0.000, 0.889, 0.878], v_f1=0.633\n",
      "05/30 19:22:51 [INFO] steps=1318/2376, v_loss=0.5411, v_f1r=[0.911, 0.968, 0.726, 0.000, 0.000, 0.915, 0.863], v_f1=0.626\n",
      "05/30 19:23:51 [INFO] steps=1649/2376, v_loss=0.5138, v_f1r=[0.909, 0.968, 0.702, 0.000, 0.950, 0.915, 0.853], v_f1=0.757\n",
      "05/30 19:24:51 [INFO] steps=1980/2376, v_loss=0.4999, v_f1r=[0.908, 0.967, 0.670, 0.690, 0.957, 0.914, 0.841], v_f1=0.849\n",
      "05/30 19:25:51 [INFO] steps=2312/2376, v_loss=0.5040, v_f1r=[0.905, 0.966, 0.625, 0.886, 0.952, 0.908, 0.827], v_f1=0.867\n",
      "05/30 19:26:03 [INFO] eps=3, steps=8131/32524, lr=2.18e-05, t_loss=0.5373, v_f1r=[0.904, 0.966, 0.618, 0.893, 0.951, 0.907, 0.825], v_loss=0.5045*, v_f1=0.8664*\n",
      "05/30 19:26:25 [INFO] eps=3, steps=8140/32524, lr=2.18e-05, t_loss=0.5541, t_f1r=[0.963, 0.977, 0.667, 0.769, 0.909, 0.791, 0.812], t_f1=0.841\n",
      "05/30 19:27:26 [INFO] eps=3, steps=8370/32524, lr=2.17e-05, t_loss=0.5486, t_f1r=[0.902, 0.955, 0.806, 0.831, 0.916, 0.856, 0.870], t_f1=0.877\n",
      "05/30 19:28:28 [INFO] eps=3, steps=8600/32524, lr=2.16e-05, t_loss=0.5534, t_f1r=[0.896, 0.958, 0.815, 0.842, 0.917, 0.847, 0.853], t_f1=0.875\n",
      "05/30 19:29:30 [INFO] eps=3, steps=8830/32524, lr=2.14e-05, t_loss=0.5550, t_f1r=[0.895, 0.962, 0.821, 0.843, 0.917, 0.844, 0.858], t_f1=0.877\n",
      "05/30 19:30:31 [INFO] eps=3, steps=9060/32524, lr=2.11e-05, t_loss=0.5569, t_f1r=[0.895, 0.962, 0.819, 0.839, 0.919, 0.844, 0.854], t_f1=0.876\n",
      "05/30 19:31:33 [INFO] eps=3, steps=9290/32524, lr=2.08e-05, t_loss=0.5597, t_f1r=[0.891, 0.964, 0.812, 0.840, 0.919, 0.843, 0.852], t_f1=0.874\n",
      "05/30 19:32:35 [INFO] eps=3, steps=9520/32524, lr=2.04e-05, t_loss=0.5573, t_f1r=[0.890, 0.962, 0.812, 0.842, 0.919, 0.847, 0.854], t_f1=0.875\n",
      "05/30 19:33:36 [INFO] eps=3, steps=9750/32524, lr=1.99e-05, t_loss=0.5576, t_f1r=[0.887, 0.962, 0.811, 0.842, 0.918, 0.844, 0.853], t_f1=0.874\n",
      "05/30 19:34:38 [INFO] eps=3, steps=9980/32524, lr=1.94e-05, t_loss=0.5590, t_f1r=[0.886, 0.963, 0.810, 0.839, 0.917, 0.844, 0.851], t_f1=0.873\n",
      "05/30 19:35:40 [INFO] eps=3, steps=10210/32524, lr=1.88e-05, t_loss=0.5583, t_f1r=[0.886, 0.963, 0.811, 0.840, 0.916, 0.845, 0.849], t_f1=0.873\n",
      "05/30 19:36:42 [INFO] eps=3, steps=10440/32524, lr=1.81e-05, t_loss=0.5573, t_f1r=[0.887, 0.964, 0.812, 0.841, 0.917, 0.844, 0.849], t_f1=0.873\n",
      "05/30 19:37:44 [INFO] eps=3, steps=10670/32524, lr=1.74e-05, t_loss=0.5560, t_f1r=[0.887, 0.964, 0.813, 0.843, 0.918, 0.842, 0.850], t_f1=0.874\n",
      "05/30 19:38:46 [INFO] eps=3, steps=10900/32524, lr=1.66e-05, t_loss=0.5534, t_f1r=[0.889, 0.965, 0.814, 0.845, 0.917, 0.843, 0.850], t_f1=0.875\n",
      "05/30 19:39:48 [INFO] eps=3, steps=11130/32524, lr=1.58e-05, t_loss=0.5518, t_f1r=[0.889, 0.965, 0.816, 0.845, 0.918, 0.844, 0.853], t_f1=0.876\n",
      "05/30 19:40:50 [INFO] eps=3, steps=11360/32524, lr=1.5e-05, t_loss=0.5501, t_f1r=[0.890, 0.966, 0.817, 0.846, 0.918, 0.845, 0.853], t_f1=0.876\n",
      "05/30 19:41:52 [INFO] eps=3, steps=11590/32524, lr=1.42e-05, t_loss=0.5492, t_f1r=[0.890, 0.966, 0.819, 0.847, 0.919, 0.845, 0.854], t_f1=0.877\n",
      "05/30 19:42:54 [INFO] eps=3, steps=11820/32524, lr=1.33e-05, t_loss=0.5480, t_f1r=[0.891, 0.966, 0.820, 0.848, 0.919, 0.845, 0.855], t_f1=0.878\n",
      "05/30 19:43:56 [INFO] eps=3, steps=12050/32524, lr=1.24e-05, t_loss=0.5469, t_f1r=[0.891, 0.966, 0.821, 0.849, 0.920, 0.846, 0.855], t_f1=0.878\n",
      "05/30 19:44:59 [INFO] eps=3, steps=12280/32524, lr=1.16e-05, t_loss=0.5452, t_f1r=[0.892, 0.967, 0.823, 0.849, 0.920, 0.847, 0.856], t_f1=0.879\n",
      "05/30 19:46:01 [INFO] eps=3, steps=12510/32524, lr=1.07e-05, t_loss=0.5427, t_f1r=[0.894, 0.967, 0.824, 0.849, 0.920, 0.848, 0.857], t_f1=0.880\n",
      "05/30 19:47:03 [INFO] eps=3, steps=12740/32524, lr=9.83e-06, t_loss=0.5409, t_f1r=[0.894, 0.968, 0.825, 0.849, 0.921, 0.848, 0.858], t_f1=0.881\n",
      "05/30 19:48:05 [INFO] eps=3, steps=12970/32524, lr=8.98e-06, t_loss=0.5399, t_f1r=[0.896, 0.968, 0.825, 0.849, 0.921, 0.849, 0.858], t_f1=0.881\n",
      "05/30 19:49:08 [INFO] eps=3, steps=13200/32524, lr=8.15e-06, t_loss=0.5390, t_f1r=[0.895, 0.968, 0.826, 0.850, 0.922, 0.849, 0.860], t_f1=0.881\n",
      "05/30 19:50:10 [INFO] eps=3, steps=13430/32524, lr=7.35e-06, t_loss=0.5381, t_f1r=[0.896, 0.968, 0.827, 0.851, 0.921, 0.850, 0.860], t_f1=0.882\n",
      "05/30 19:51:13 [INFO] eps=3, steps=13660/32524, lr=6.59e-06, t_loss=0.5362, t_f1r=[0.897, 0.969, 0.828, 0.851, 0.922, 0.850, 0.860], t_f1=0.882\n",
      "05/30 19:52:15 [INFO] eps=3, steps=13890/32524, lr=5.87e-06, t_loss=0.5342, t_f1r=[0.898, 0.969, 0.829, 0.852, 0.922, 0.852, 0.862], t_f1=0.884\n",
      "05/30 19:53:18 [INFO] eps=3, steps=14120/32524, lr=5.2e-06, t_loss=0.5324, t_f1r=[0.899, 0.970, 0.831, 0.854, 0.922, 0.852, 0.863], t_f1=0.884\n",
      "05/30 19:54:20 [INFO] eps=3, steps=14350/32524, lr=4.58e-06, t_loss=0.5307, t_f1r=[0.900, 0.970, 0.832, 0.854, 0.923, 0.853, 0.863], t_f1=0.885\n",
      "05/30 19:55:23 [INFO] eps=3, steps=14580/32524, lr=4.02e-06, t_loss=0.5283, t_f1r=[0.901, 0.970, 0.834, 0.855, 0.924, 0.854, 0.864], t_f1=0.886\n",
      "05/30 19:56:25 [INFO] eps=3, steps=14810/32524, lr=3.52e-06, t_loss=0.5260, t_f1r=[0.901, 0.971, 0.836, 0.856, 0.924, 0.855, 0.865], t_f1=0.887\n",
      "05/30 19:57:28 [INFO] eps=3, steps=15040/32524, lr=3.08e-06, t_loss=0.5245, t_f1r=[0.902, 0.971, 0.836, 0.857, 0.925, 0.856, 0.865], t_f1=0.887\n",
      "05/30 19:58:30 [INFO] eps=3, steps=15270/32524, lr=2.72e-06, t_loss=0.5235, t_f1r=[0.902, 0.971, 0.837, 0.858, 0.925, 0.857, 0.865], t_f1=0.888\n",
      "05/30 19:59:33 [INFO] eps=3, steps=15500/32524, lr=2.43e-06, t_loss=0.5221, t_f1r=[0.903, 0.971, 0.838, 0.858, 0.925, 0.857, 0.865], t_f1=0.888\n",
      "05/30 20:00:36 [INFO] eps=3, steps=15730/32524, lr=2.21e-06, t_loss=0.5204, t_f1r=[0.904, 0.971, 0.840, 0.858, 0.926, 0.858, 0.866], t_f1=0.889\n",
      "05/30 20:01:36 [INFO] eps=3, steps=15950/32524, lr=2.07e-06, t_loss=0.5193, t_f1r=[0.904, 0.972, 0.840, 0.858, 0.926, 0.859, 0.867], t_f1=0.889\n",
      "05/30 20:02:36 [INFO] eps=3, steps=16170/32524, lr=2.01e-06, t_loss=0.5180, t_f1r=[0.904, 0.972, 0.841, 0.859, 0.926, 0.860, 0.867], t_f1=0.890\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad7dcceb96f4dff9850e4f89e86ae23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2376 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/30 20:04:01 [INFO] steps=323/2376, v_loss=0.4846, v_f1r=[0.953, 0.000, 0.000, 0.000, 0.000, 0.000, 0.875], v_f1=0.261\n",
      "05/30 20:05:01 [INFO] steps=654/2376, v_loss=0.4739, v_f1r=[0.947, 0.979, 0.000, 0.000, 0.000, 0.000, 0.911], v_f1=0.405\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "dt_str = datetime.now().strftime('%m%d%H%M')\n",
    "\n",
    "for fold_idx, (train_index, valid_index) in enumerate(skf.split(train_df, train_df['rock_type_int'])):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    logger.info(f'{fold_idx=} started')\n",
    "    import wandb\n",
    "    run = wandb.init(\n",
    "        name=f'fold{fold_idx+1}_{CFG[\"MODEL_NAME\"].split(\"/\")[1].split(\"-\")[0]}_{dt_str}',\n",
    "        config=CFG,\n",
    "        reinit=True)\n",
    "    \n",
    "    train_fold_df = train_df.iloc[train_index,:].copy().reset_index(drop=True)\n",
    "    valid_fold_df = train_df.iloc[valid_index,:].copy().reset_index(drop=True)\n",
    "\n",
    "    model = create_model(CFG['MODEL_NAME'])\n",
    "    if (CFG['GRADIENT_CHECKPOINT']==True) and ('set_grad_checkpointing' in dir(model)):\n",
    "        model.set_grad_checkpointing() ## for memory_efficient..\n",
    "        logger.info('grad_checkpointing : True')\n",
    "    else:\n",
    "        model.compile() #\n",
    "        logger.info('model compiled')\n",
    "    \n",
    "    ## wrap model\n",
    "    optimizer_class_ = getattr(torch.optim, CFG['OPTIMIZER'] )\n",
    "    logger.info(f'create optimizer : {optimizer_class_.__module__}')\n",
    "    optimizer = optimizer_class_(\n",
    "        model.parameters(),\n",
    "        lr=CFG['LR'][0],\n",
    "        weight_decay=0.001,  ## default는 0.01이며, 논문은 0.001임.\n",
    "    )\n",
    "    import bitsandbytes as bnb\n",
    "    optimizer = bnb.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=CFG['LR'][0],\n",
    "        weight_decay=0.001,  ## default는 0.01이며, 논문은 0.001임.\n",
    "        optim_bits=8,\n",
    "    )\n",
    "    \n",
    "    model = train( \n",
    "        model, optimizer,\n",
    "        train_fold_df,\n",
    "        valid_fold_df, \n",
    "        # val_loader,\n",
    "        # scheduler, \n",
    "        device,\n",
    "        use_amp=(CFG['PRECISION'] == '16'),\n",
    "        filename = f'./ckpt/{CFG[\"MODEL_NAME\"].split(\"/\")[1].split(\"-\")[0]}-fold_idx={fold_idx}-' + 'epoch={epoch:02d}-val_loss={val_loss:.4f}-val_score={val_score:.4f}',\n",
    "    )\n",
    "    \n",
    "    model = None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    logger.info(f'{fold_idx=} finished')\n",
    "    run.finish()\n",
    "    \n",
    "    try:\n",
    "        # !python ~/send_telegram.py 'fold_idx={fold_idx} finished'\n",
    "        last_chpt_info = !ls -t ./ckpt/ | head -n1\n",
    "        last_chpt_info = ','.join( last_chpt_info[0][:-5].split('-')[1:] )\n",
    "        !python ~/send_telegram.py {last_chpt_info}\n",
    "    except:\n",
    "        pass\n",
    "    # 1pass만..\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ec20b-f3a4-4785-a5c0-11bd2ac9f069",
   "metadata": {},
   "source": [
    "# 모델 앙상블 및 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1c8a6-d2b7-4ec4-bcff-3eb034435ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold_df = None\n",
    "valid_fold_df = None\n",
    "train_dataset = None\n",
    "train_loader = None\n",
    "val_dataset = None\n",
    "val_loader = None\n",
    "\n",
    "optimizer = None\n",
    "scheduler = None\n",
    "model = None\n",
    "\n",
    "run = None\n",
    "    \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab45007-1311-4504-92ea-62dfe347c653",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630039b-1f83-46f0-bd50-d52015bdf62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "ckpt_df = pd.DataFrame({'fname':glob('./ckpt/*.ckpt')})\n",
    "ckpt_df['mtime'] = ckpt_df.fname.apply(lambda x: int(os.stat(x).st_mtime))\n",
    "ckpt_df['model_name'] = ckpt_df.fname.apply(lambda x: re.search(r'./ckpt/(.*?)-fold',x)[1])\n",
    "ckpt_df['img_size'] = ckpt_df.fname.apply(lambda x: int(re.search(r'patch[0-9]+_([0-9]+)', x + 'patch0_0')[1]) )\n",
    "ckpt_df['is_ema'] = ckpt_df.fname.str.endswith('ema.ckpt').astype(int)\n",
    "ckpt_df['fold_idx'] = ckpt_df.fname.apply(lambda x: int(re.search(r'fold_idx=([0-9])-',x)[1]))\n",
    "ckpt_df['val_loss'] = ckpt_df.fname.apply(lambda x: float(re.search(r'val_loss=(0\\.[0-9]+)', x)[1]) )\n",
    "ckpt_df['val_score'] = ckpt_df.fname.apply(lambda x: float(re.search(r'val_score=(0\\.[0-9]+)', x)[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962a8a45-25ea-4967-8870-053d6fa0c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_df = ckpt_df[ckpt_df.img_size != 0] # [ckpt_df.is_ema == 0]\n",
    "ckpt_df = ckpt_df.sort_values('mtime',ascending=False).reset_index(drop=True)\n",
    "MAX_SIZE = 1 ## 상위 4개..\n",
    "ckpt_indexes = ckpt_df[ ckpt_df.fold_idx==ckpt_df.fold_idx.max() ].index[:MAX_SIZE]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29306d0e-0a1c-4837-81e0-59fb5fd7a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "preds_score = []\n",
    "\n",
    "for ckpt_start_index in ckpt_indexes:\n",
    "    # 메모리 초기화..\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    logger.info(f'{ckpt_df.fname[ckpt_start_index]} loading')\n",
    "    ## imagesize\n",
    "    CFG['IMG_SIZE'] = int(ckpt_df.img_size[ckpt_start_index])\n",
    "    assert CFG['IMG_SIZE'] in ( 196, 224, 448 )\n",
    "    logger.info(CFG['IMG_SIZE'])\n",
    "\n",
    "    test_dataset = CustomDataset(\n",
    "        test_df['img_path'].values, None, \n",
    "        interpolation=CFG['INTERPOLATION'], load_img_size=CFG['IMG_SIZE'],\n",
    "        shuffle=False, transforms=test_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE']*2, shuffle=False, num_workers=4)\n",
    "\n",
    "    model_name = ckpt_df.model_name[ckpt_start_index]\n",
    "    model = create_model(model_name)\n",
    "    # 관련이 없을 듯 한데, 그래도 영향이 발생하지 않도록 \n",
    "    if (CFG['GRADIENT_CHECKPOINT']==True) and ('set_grad_checkpointing' in dir(model)):\n",
    "        model.set_grad_checkpointing() ## for memory_efficient..\n",
    "    #     # logger.info('grad_checkpointing : True')\n",
    "    else:\n",
    "        model.compile() #\n",
    "    #     # logger.info('model compiled')\n",
    "    \n",
    "    if ckpt_df.is_ema[ckpt_start_index]:\n",
    "        model = torch.optim.swa_utils.AveragedModel(model)\n",
    "    #-----------------------------\n",
    "    for i in range(ckpt_start_index, ckpt_start_index + ckpt_df.fold_idx.max() + 1 ):\n",
    "        checkpoint_path = ckpt_df.fname[i]\n",
    "        logger.info(f'{checkpoint_path} loading')\n",
    "        model.load_state_dict( torch.load(checkpoint_path)['model'] )\n",
    "        \n",
    "        preds_score.append( ckpt_df.val_score[i] )\n",
    "        preds.append( prediction(model, test_loader, device) )\n",
    "    \n",
    "preds = np.array(preds)\n",
    "preds_score = np.array(preds_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a48ae-0174-44ff-8a9a-10aefcf6bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ### 가중치 평균값..\n",
    "preds_labels = le.inverse_transform(preds.sum(0).argmax(-1))\n",
    "print(preds_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbfd7f8-701e-498a-9558-c62dcddd4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['rock_type'] = preds_labels\n",
    "from datetime import datetime\n",
    "dt_str = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "submit.to_csv(f'./basslibrary_submit_{dt_str}.csv', index=False)\n",
    "logger.info(f'./basslibrary_submit_{dt_str}.csv saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b84a5-8a71-4f53-af2c-adfb9439bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_value_counts = str(submit['rock_type'].value_counts())\n",
    "!python ~/send_telegram.py f'submit created: {submit_value_counts}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
